{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# On the Limits of Learned Importance Scoring for KV Cache Compression\n",
    "\n",
    "## Evaluation Experiments Notebook\n",
    "\n",
    "This notebook contains all experiments for our negative results paper investigating learned KV cache compression.\n",
    "\n",
    "**Key Finding:** Despite architectural sophistication (multi-horizon lookahead, cross-attention, confidence weighting), our learned scorer (SIP, 1.7M parameters) does **not** outperform simple heuristics—and random selection often performs comparably.\n",
    "\n",
    "### Main Results Summary\n",
    "- **Position-Heuristic** wins at aggressive compression (10%, 25%)\n",
    "- **Prefill-Attn** wins at moderate compression (50%, 75%)  \n",
    "- **SIP ≈ Random** (no statistically significant difference)\n",
    "\n",
    "### Experiments Included\n",
    "1. Multi-seed perplexity evaluation (5 seeds, 95% CI, paired t-tests)\n",
    "2. Task-based evaluation (QA accuracy, Needle-in-Haystack)\n",
    "3. Component ablation study\n",
    "4. Lookahead prediction accuracy\n",
    "5. Confidence calibration analysis\n",
    "\n",
    "**Hardware:** A100 GPU recommended  \n",
    "**Runtime:** ~2-4 hours for full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets accelerate\n",
    "!pip install -q scipy scikit-learn matplotlib seaborn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass, asdict\nfrom collections import deque\nfrom tqdm.auto import tqdm\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport math\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass SIPConfig:\n    hidden_dim: int = 2048\n    head_dim: int = 64\n    num_heads: int = 32\n    num_kv_heads: int = 4\n    lookahead_steps: int = 8\n    speculation_horizon: int = 16\n    encoder_hidden_dim: int = 256\n    encoder_layers: int = 2\n    predictor_hidden_dim: int = 128\n    num_predictor_heads: int = 4\n    dropout: float = 0.1\n    use_confidence_weighting: bool = True\n    confidence_temperature: float = 1.0\n    discount_factor: float = 0.95\n\n\nTRAINING_CONFIG = {\n    \"num_train_samples\": 5000,\n    \"num_val_samples\": 500,\n    \"batch_size\": 4,\n    \"epochs\": 10,\n    \"lr\": 1e-4,\n    \"weight_decay\": 0.01,\n    \"warmup_steps\": 500,\n    \"gradient_clip\": 1.0,\n    \"accumulation_steps\": 4,\n    \"rollout_length\": 32,\n    \"initial_lookahead\": 1,\n    \"max_lookahead\": 8,\n    \"curriculum_warmup\": 5,\n    \"kl_weight\": 1.0,\n    \"ranking_weight\": 0.5,\n    \"confidence_weight\": 0.3,\n    \"temporal_weight\": 0.1,\n}"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Improved Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "class ImprovedSIPLoss(nn.Module):\n    def __init__(\n        self,\n        kl_weight: float = 1.0,\n        ranking_weight: float = 0.5,\n        confidence_weight: float = 0.3,\n        temporal_weight: float = 0.1,\n        focal_gamma: float = 2.0,\n        temperature: float = 10.0,\n    ):\n        super().__init__()\n        self.kl_weight = kl_weight\n        self.ranking_weight = ranking_weight\n        self.confidence_weight = confidence_weight\n        self.temporal_weight = temporal_weight\n        self.focal_gamma = focal_gamma\n        self.temperature = temperature\n\n    def forward(\n        self,\n        predicted_importance: torch.Tensor,\n        confidence: torch.Tensor,\n        future_attention: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        losses = {}\n        batch, heads, seq_len, lookahead = predicted_importance.shape\n        device = predicted_importance.device\n\n        rollout_steps = future_attention.shape[2]\n        actual_lookahead = min(lookahead, rollout_steps)\n\n        future_attention_truncated = future_attention[:, :, :actual_lookahead, :]\n        target_soft_ranks = F.softmax(future_attention_truncated * self.temperature, dim=-1)\n        target_soft_ranks = target_soft_ranks.permute(0, 1, 3, 2)\n\n        predicted_truncated = predicted_importance[:, :, :, :actual_lookahead]\n        confidence_truncated = confidence[:, :, :, :actual_lookahead]\n\n        pred_log_probs = F.log_softmax(predicted_truncated * self.temperature, dim=2)\n        kl_loss = F.kl_div(pred_log_probs, target_soft_ranks, reduction='batchmean')\n        losses['kl'] = self.kl_weight * kl_loss\n\n        pred_flat = predicted_truncated[:, :, :, 0].reshape(-1, seq_len)\n        target_flat = target_soft_ranks[:, :, :, 0].reshape(-1, seq_len)\n\n        k = min(32, seq_len // 4)\n        if k > 1:\n            _, top_idx = target_flat.topk(k, dim=-1)\n            _, bot_idx = target_flat.topk(k, dim=-1, largest=False)\n            pred_top = pred_flat.gather(1, top_idx).mean(dim=1)\n            pred_bot = pred_flat.gather(1, bot_idx).mean(dim=1)\n            margin = 0.2\n            ranking_loss = F.relu(margin - (pred_top - pred_bot)).mean()\n            losses['ranking'] = self.ranking_weight * ranking_loss\n        else:\n            losses['ranking'] = torch.tensor(0.0, device=device)\n\n        pred_probs = F.softmax(predicted_truncated * self.temperature, dim=2)\n        pred_error = (pred_probs - target_soft_ranks).abs().mean(dim=-1)\n        target_confidence = (1.0 - pred_error).clamp(0, 1)\n\n        conf_mean = confidence_truncated.mean(dim=-1)\n        pt = conf_mean * target_confidence + (1 - conf_mean) * (1 - target_confidence)\n        focal_weight = (1 - pt) ** self.focal_gamma\n        conf_loss = (focal_weight * F.mse_loss(conf_mean, target_confidence.detach(), reduction='none')).mean()\n        losses['confidence'] = self.confidence_weight * conf_loss\n\n        if actual_lookahead > 1:\n            temporal_diff = (predicted_truncated[:, :, :, 1:] - predicted_truncated[:, :, :, :-1]).abs()\n            losses['temporal'] = self.temporal_weight * temporal_diff.mean()\n        else:\n            losses['temporal'] = torch.tensor(0.0, device=device)\n\n        losses['total'] = sum(v for k, v in losses.items() if k != 'total')\n        return losses"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csgx22yfl6m",
   "metadata": {},
   "outputs": [],
   "source": "class ListMLELoss(nn.Module):\n    def __init__(self, temperature: float = 1.0, eps: float = 1e-10):\n        super().__init__()\n        self.temperature = temperature\n        self.eps = eps\n    \n    def forward(\n        self,\n        predicted_scores: torch.Tensor,\n        target_scores: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        if predicted_scores.dim() == 3:\n            predicted_scores = predicted_scores.mean(dim=1)\n            target_scores = target_scores.mean(dim=1)\n        \n        batch_size, seq_len = predicted_scores.shape\n        device = predicted_scores.device\n        \n        _, sorted_indices = target_scores.sort(dim=-1, descending=True)\n        sorted_preds = predicted_scores.gather(1, sorted_indices) / self.temperature\n        \n        if mask is not None:\n            if mask.dim() == 1:\n                mask = mask.unsqueeze(0).expand(batch_size, -1)\n            sorted_mask = mask.gather(1, sorted_indices)\n            sorted_preds = sorted_preds.masked_fill(~sorted_mask.bool(), float('-inf'))\n        \n        max_pred = sorted_preds.max(dim=-1, keepdim=True)[0]\n        shifted = sorted_preds - max_pred\n        \n        cumsumexp = torch.zeros(batch_size, seq_len + 1, device=device)\n        for i in range(seq_len - 1, -1, -1):\n            cumsumexp[:, i] = torch.log(torch.exp(shifted[:, i]) + torch.exp(cumsumexp[:, i + 1]) + self.eps)\n        \n        log_probs = shifted - cumsumexp[:, :-1]\n        \n        if mask is not None:\n            loss = -(log_probs * sorted_mask.float()).sum(dim=-1) / (sorted_mask.float().sum(dim=-1) + self.eps)\n        else:\n            loss = -log_probs.mean(dim=-1)\n        \n        return loss.mean()\n\n\nclass PairwiseRankingLoss(nn.Module):\n    def __init__(self, margin: float = 0.1, num_pairs: int = 256):\n        super().__init__()\n        self.margin = margin\n        self.num_pairs = num_pairs\n    \n    def forward(\n        self,\n        predicted_scores: torch.Tensor,\n        target_scores: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        if predicted_scores.dim() == 3:\n            predicted_scores = predicted_scores.mean(dim=1)\n            target_scores = target_scores.mean(dim=1)\n        \n        batch_size, seq_len = predicted_scores.shape\n        device = predicted_scores.device\n        num_pairs = min(self.num_pairs, seq_len * (seq_len - 1) // 2)\n        \n        total_loss = 0.0\n        valid_pairs = 0\n        \n        for b in range(batch_size):\n            idx1 = torch.randint(0, seq_len, (num_pairs,), device=device)\n            idx2 = torch.randint(0, seq_len, (num_pairs,), device=device)\n            \n            pred1 = predicted_scores[b, idx1]\n            pred2 = predicted_scores[b, idx2]\n            target1 = target_scores[b, idx1]\n            target2 = target_scores[b, idx2]\n            \n            target_diff = target1 - target2\n            sign = torch.sign(target_diff)\n            pred_diff = pred1 - pred2\n            pair_loss = F.relu(self.margin - sign * pred_diff)\n            \n            meaningful = target_diff.abs() > 0.01\n            if meaningful.any():\n                total_loss += pair_loss[meaningful].mean()\n                valid_pairs += 1\n        \n        if valid_pairs == 0:\n            return torch.tensor(0.0, device=device)\n        \n        return total_loss / valid_pairs\n\n\nclass EnhancedSIPLoss(nn.Module):\n    def __init__(\n        self,\n        listmle_weight: float = 1.0,\n        pairwise_weight: float = 0.5,\n        topk_weight: float = 0.3,\n        confidence_weight: float = 0.2,\n        focal_gamma: float = 2.0,\n        top_k_ratio: float = 0.25,\n    ):\n        super().__init__()\n        self.listmle_weight = listmle_weight\n        self.pairwise_weight = pairwise_weight\n        self.topk_weight = topk_weight\n        self.confidence_weight = confidence_weight\n        self.focal_gamma = focal_gamma\n        self.top_k_ratio = top_k_ratio\n        \n        self.listmle = ListMLELoss()\n        self.pairwise = PairwiseRankingLoss()\n    \n    def forward(\n        self,\n        predicted_importance: torch.Tensor,\n        confidence: torch.Tensor,\n        future_attention: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n    ) -> Dict[str, torch.Tensor]:\n        losses = {}\n        batch, heads, seq_len, lookahead = predicted_importance.shape\n        device = predicted_importance.device\n        \n        rollout_steps = future_attention.shape[2]\n        actual_lookahead = min(lookahead, rollout_steps)\n        \n        pred = predicted_importance[:, :, :, 0]\n        target = future_attention[:, :, 0, :]\n        target = target / (target.sum(dim=-1, keepdim=True) + 1e-8)\n        \n        pred_flat = pred.view(batch, -1)\n        target_flat = target.view(batch, -1)\n        losses['listmle'] = self.listmle_weight * self.listmle(pred_flat, target_flat)\n        losses['pairwise'] = self.pairwise_weight * self.pairwise(pred_flat, target_flat)\n        \n        k = max(1, int(seq_len * self.top_k_ratio))\n        _, pred_topk = pred.view(batch, -1).topk(k * heads, dim=-1)\n        _, target_topk = target.view(batch, -1).topk(k * heads, dim=-1)\n        \n        pred_mask = torch.zeros(batch, heads * seq_len, device=device)\n        target_mask = torch.zeros(batch, heads * seq_len, device=device)\n        pred_mask.scatter_(1, pred_topk, 1.0)\n        target_mask.scatter_(1, target_topk, 1.0)\n        \n        intersection = (pred_mask * target_mask).sum(dim=-1)\n        recall = intersection / (target_mask.sum(dim=-1) + 1e-8)\n        losses['topk_recall'] = self.topk_weight * (1.0 - recall.mean())\n        \n        conf = confidence[:, :, :, 0]\n        pred_normalized = pred / (pred.max(dim=-1, keepdim=True)[0] + 1e-8)\n        error = (pred_normalized - target).abs().mean(dim=1)\n        accuracy = (1.0 - error).clamp(0, 1)\n        \n        conf_mean = conf.mean(dim=1)\n        pt = conf_mean * accuracy + (1 - conf_mean) * (1 - accuracy)\n        focal_weight = (1 - pt) ** self.focal_gamma\n        conf_loss = (focal_weight * F.mse_loss(conf_mean, accuracy.detach(), reduction='none')).mean()\n        losses['confidence'] = self.confidence_weight * conf_loss\n        \n        losses['total'] = sum(v for k, v in losses.items() if k != 'total')\n        return losses"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. SIP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "class TemporalPositionEncoding(nn.Module):\n    def __init__(self, dim: int, max_len: int = 8192, max_lookahead: int = 64):\n        super().__init__()\n        self.dim = dim\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))\n        pe = torch.zeros(max_len, dim)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n        relative = torch.arange(-max_len, max_len).unsqueeze(1)\n        rel_pe = torch.zeros(2 * max_len, dim)\n        rel_pe[:, 0::2] = torch.sin(relative * div_term)\n        rel_pe[:, 1::2] = torch.cos(relative * div_term)\n        self.register_buffer('rel_pe', rel_pe)\n\n        lookahead = torch.arange(max_lookahead).unsqueeze(1)\n        look_pe = torch.zeros(max_lookahead, dim // 2)\n        look_div = torch.exp(torch.arange(0, dim // 2, 2) * (-math.log(10000.0) / (dim // 2)))\n        look_pe[:, 0::2] = torch.sin(lookahead * look_div)\n        look_pe[:, 1::2] = torch.cos(lookahead * look_div)\n        self.register_buffer('look_pe', look_pe)\n\n    def forward(self, positions: torch.Tensor, current_step: int, lookahead_step: int = 0) -> torch.Tensor:\n        batch, seq_len = positions.shape\n        abs_enc = self.pe[positions.clamp(0, self.pe.shape[0] - 1)]\n        rel_positions = current_step - positions + self.rel_pe.shape[0] // 2\n        rel_positions = rel_positions.clamp(0, self.rel_pe.shape[0] - 1)\n        rel_enc = self.rel_pe[rel_positions]\n        look_enc = self.look_pe[min(lookahead_step, self.look_pe.shape[0] - 1)]\n        look_enc = look_enc.unsqueeze(0).unsqueeze(0).expand(batch, seq_len, -1)\n        combined = abs_enc + rel_enc\n        combined[:, :, :look_enc.shape[-1]] = combined[:, :, :look_enc.shape[-1]] + look_enc\n        return combined\n\n\nclass KeyValueEncoder(nn.Module):\n    def __init__(self, config: SIPConfig):\n        super().__init__()\n        self.config = config\n        self.key_encoder = nn.Sequential(\n            nn.Linear(config.head_dim, config.encoder_hidden_dim),\n            nn.LayerNorm(config.encoder_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(config.encoder_hidden_dim, config.encoder_hidden_dim),\n        )\n        self.value_encoder = nn.Sequential(\n            nn.Linear(config.head_dim, config.encoder_hidden_dim),\n            nn.LayerNorm(config.encoder_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(config.encoder_hidden_dim, config.encoder_hidden_dim),\n        )\n        self.kv_attention = nn.MultiheadAttention(\n            embed_dim=config.encoder_hidden_dim,\n            num_heads=4,\n            dropout=config.dropout,\n            batch_first=True,\n        )\n        self.output_proj = nn.Linear(config.encoder_hidden_dim * 2, config.encoder_hidden_dim)\n\n    def forward(self, keys: torch.Tensor, values: torch.Tensor) -> torch.Tensor:\n        batch, heads, seq_len, head_dim = keys.shape\n        keys_flat = keys.view(batch * heads, seq_len, head_dim)\n        values_flat = values.view(batch * heads, seq_len, head_dim)\n        key_enc = self.key_encoder(keys_flat)\n        value_enc = self.value_encoder(values_flat)\n        kv_combined, _ = self.kv_attention(key_enc, value_enc, value_enc)\n        combined = torch.cat([key_enc, kv_combined], dim=-1)\n        output = self.output_proj(combined)\n        return output.view(batch, heads, seq_len, -1)\n\n\nclass SpeculativePredictor(nn.Module):\n    def __init__(self, config: SIPConfig):\n        super().__init__()\n        self.config = config\n        self.temporal_encoding = TemporalPositionEncoding(\n            config.encoder_hidden_dim, max_lookahead=config.speculation_horizon\n        )\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=config.encoder_hidden_dim,\n            nhead=config.num_predictor_heads,\n            dim_feedforward=config.predictor_hidden_dim * 4,\n            dropout=config.dropout,\n            activation='gelu',\n            batch_first=True,\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.encoder_layers)\n        self.lookahead_queries = nn.Parameter(\n            torch.randn(config.lookahead_steps, config.encoder_hidden_dim) * 0.02\n        )\n        self.importance_head = nn.Sequential(\n            nn.LayerNorm(config.encoder_hidden_dim),\n            nn.Linear(config.encoder_hidden_dim, config.predictor_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(config.predictor_hidden_dim, 1),\n        )\n        self.confidence_head = nn.Sequential(\n            nn.LayerNorm(config.encoder_hidden_dim),\n            nn.Linear(config.encoder_hidden_dim, config.predictor_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(config.predictor_hidden_dim, 1),\n        )\n        self.head_bias = nn.Parameter(torch.zeros(config.num_kv_heads))\n        self.temperature = nn.Parameter(torch.ones(1))\n\n    def forward(\n        self, kv_features: torch.Tensor, positions: torch.Tensor,\n        current_step: int, lookahead_steps: Optional[int] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if lookahead_steps is None:\n            lookahead_steps = self.config.lookahead_steps\n        batch, heads, seq_len, hidden = kv_features.shape\n        dtype = kv_features.dtype\n        all_importance, all_confidence = [], []\n\n        for step in range(lookahead_steps):\n            temp_enc = self.temporal_encoding(positions, current_step, step)\n            temp_enc = temp_enc.unsqueeze(1).expand(-1, heads, -1, -1)\n            features = kv_features + temp_enc.to(dtype)\n            query = self.lookahead_queries[min(step, len(self.lookahead_queries)-1)]\n            query = query.view(1, 1, 1, -1).expand(batch, heads, seq_len, -1)\n            features = features + query.to(dtype)\n            features_flat = features.view(batch * heads, seq_len, hidden)\n            transformed = self.transformer(features_flat.float()).to(dtype)\n            importance = self.importance_head(transformed.float()).squeeze(-1)\n            confidence = self.confidence_head(transformed.float()).squeeze(-1)\n            importance = importance.view(batch, heads, seq_len)\n            confidence = confidence.view(batch, heads, seq_len)\n            importance = importance + self.head_bias.view(1, -1, 1).to(dtype)\n            importance = torch.sigmoid(importance / self.temperature)\n            confidence = torch.sigmoid(confidence)\n            all_importance.append(importance)\n            all_confidence.append(confidence)\n\n        return torch.stack(all_importance, dim=-1), torch.stack(all_confidence, dim=-1)\n\n\nclass SpeculativeImportanceScorer(nn.Module):\n    def __init__(self, config: SIPConfig):\n        super().__init__()\n        self.config = config\n        self.kv_encoder = KeyValueEncoder(config)\n        self.predictor = SpeculativePredictor(config)\n\n    def forward(\n        self, keys: torch.Tensor, values: torch.Tensor, positions: torch.Tensor,\n        current_step: Optional[int] = None, return_all_lookahead: bool = False,\n    ) -> torch.Tensor:\n        batch, heads, seq_len, head_dim = keys.shape\n        if current_step is None:\n            current_step = seq_len\n        kv_features = self.kv_encoder(keys.float(), values.float())\n        importance_pred, confidence = self.predictor(kv_features, positions, current_step)\n        if return_all_lookahead:\n            return importance_pred\n        weights = F.softmax(confidence / self.config.confidence_temperature, dim=-1)\n        importance = (importance_pred * weights).sum(dim=-1)\n        return importance"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. KV Cache Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "from transformers.cache_utils import DynamicCache\n\ndef get_cache_keys_values(cache, layer_idx: int = 0):\n    if isinstance(cache, DynamicCache):\n        try:\n            return cache[layer_idx]\n        except (IndexError, TypeError):\n            pass\n        if hasattr(cache, 'key_cache') and cache.key_cache:\n            return cache.key_cache[layer_idx], cache.value_cache[layer_idx]\n    elif isinstance(cache, (list, tuple)) and len(cache) > 0:\n        return cache[layer_idx]\n    raise ValueError(f\"Unknown cache format: {type(cache)}\")\n\n\ndef get_cache_length(cache) -> int:\n    if isinstance(cache, DynamicCache):\n        return len(cache)\n    elif isinstance(cache, (list, tuple)):\n        return len(cache)\n    return 0\n\n\n@dataclass\nclass CompressionConfig:\n    attention_sink_size: int = 4\n    recent_window_size: int = 64\n    min_cache_size: int = 128\n\n\nclass HardEvictionCache:\n    def __init__(self, config: CompressionConfig = None):\n        self.config = config or CompressionConfig()\n\n    def _get_keep_indices(self, importance, seq_len, retention_ratio, device, batch_size=1):\n        budget = max(int(seq_len * retention_ratio), self.config.min_cache_size)\n        budget = min(budget, seq_len)\n\n        base_mask = torch.zeros(seq_len, dtype=torch.bool, device=device)\n        sink_end = min(self.config.attention_sink_size, seq_len)\n        base_mask[:sink_end] = True\n        recent_start = max(0, seq_len - self.config.recent_window_size)\n        base_mask[recent_start:] = True\n\n        reserved = base_mask.sum().item()\n        remaining_budget = max(0, budget - reserved)\n\n        if importance.dim() == 1:\n            keep_mask = base_mask.clone()\n            if remaining_budget > 0:\n                importance_masked = importance.clone()\n                importance_masked[base_mask] = -float('inf')\n                available = (~base_mask).sum().item()\n                k = min(remaining_budget, available)\n                if k > 0:\n                    _, top_indices = importance_masked.topk(k)\n                    keep_mask[top_indices] = True\n            keep_indices = keep_mask.nonzero(as_tuple=True)[0]\n        else:\n            batch = importance.shape[0]\n            keep_masks = base_mask.unsqueeze(0).expand(batch, -1).clone()\n            \n            middle_len = seq_len - self.config.attention_sink_size - self.config.recent_window_size\n            k = min(remaining_budget, max(0, middle_len))\n            \n            if k > 0:\n                for b in range(batch):\n                    scores = importance[b].clone()\n                    scores[base_mask] = -float('inf')\n                    _, top_indices = scores.topk(k, dim=-1)\n                    keep_masks[b, top_indices] = True\n            \n            union_mask = keep_masks.any(dim=0)\n            keep_indices = union_mask.nonzero(as_tuple=True)[0]\n\n        return keep_indices.sort()[0]\n\n    def compress_cache(self, past_key_values, importance_scores, retention_ratio):\n        num_layers = get_cache_length(past_key_values)\n        if num_layers == 0:\n            return past_key_values\n\n        keys, values = get_cache_keys_values(past_key_values, 0)\n        seq_len = keys.shape[2]\n        batch_size = keys.shape[0]\n        device = keys.device\n\n        if importance_scores.dim() == 3:\n            avg_importance = importance_scores.mean(dim=1)\n        elif importance_scores.dim() == 2:\n            avg_importance = importance_scores\n        else:\n            avg_importance = importance_scores.unsqueeze(0)\n        avg_importance = avg_importance.to(device)\n\n        keep_indices = self._get_keep_indices(avg_importance, seq_len, retention_ratio, device, batch_size)\n\n        new_cache = DynamicCache()\n        for layer_idx in range(num_layers):\n            keys, values = get_cache_keys_values(past_key_values, layer_idx)\n            new_keys = keys.index_select(2, keep_indices)\n            new_values = values.index_select(2, keep_indices)\n            new_cache.update(new_keys, new_values, layer_idx)\n\n        return new_cache\n\n\nclass H2OImportanceScorer:\n    def __init__(self, decay: float = 0.9):\n        self.decay = decay\n        self.accumulated_attention = None\n\n    def reset(self):\n        self.accumulated_attention = None\n\n    def __call__(self, keys, values, attentions):\n        if attentions.dim() == 4:\n            current = attentions[:, :, -1, :]\n        else:\n            current = attentions\n        if self.accumulated_attention is None:\n            self.accumulated_attention = current\n        else:\n            if self.accumulated_attention.shape[-1] < current.shape[-1]:\n                pad_len = current.shape[-1] - self.accumulated_attention.shape[-1]\n                self.accumulated_attention = F.pad(self.accumulated_attention, (0, pad_len))\n            self.accumulated_attention = self.decay * self.accumulated_attention + current\n        return self.accumulated_attention\n\n\nclass SnapKVImportanceScorer:\n    def __init__(self, window_size=32, kernel_size=5):\n        self.window_size = window_size\n        self.kernel_size = kernel_size\n\n    def __call__(self, keys, values, attentions):\n        if attentions.dim() == 4:\n            batch, heads, query_len, kv_len = attentions.shape\n            obs_start = max(0, query_len - self.window_size)\n            obs_attn = attentions[:, :, obs_start:, :]\n            summed_attn = obs_attn.sum(dim=2)\n        else:\n            summed_attn = attentions\n        return summed_attn\n\n\nclass RecentImportanceScorer:\n    def __call__(self, keys, values, attentions):\n        seq_len = keys.shape[2]\n        device = keys.device\n        importance = torch.arange(seq_len, device=device, dtype=torch.float32) / seq_len\n        batch, heads = keys.shape[0], keys.shape[1]\n        return importance.unsqueeze(0).unsqueeze(0).expand(batch, heads, -1)\n\n\nclass RandomImportanceScorer:\n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, _ = keys.shape\n        return torch.rand(batch, heads, seq_len, device=keys.device)\n\n\nclass StreamingLLMScorer:\n    def __init__(self, sink_size=4, window_size=252):\n        self.sink_size = sink_size\n        self.window_size = window_size\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, _ = keys.shape\n        device = keys.device\n        importance = torch.zeros(batch, heads, seq_len, device=device)\n        importance[:, :, :self.sink_size] = 1.0\n        if seq_len > self.sink_size:\n            window_start = max(self.sink_size, seq_len - self.window_size)\n            importance[:, :, window_start:] = 1.0\n        return importance\n\n\nclass ExpectedAttentionScorer:\n    def __init__(self, query_history_size=32, temperature=1.0):\n        self.query_history_size = query_history_size\n        self.temperature = temperature\n        self.query_mean = None\n        self.query_cov_diag = None\n\n    def reset(self):\n        self.query_mean = None\n        self.query_cov_diag = None\n\n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n\n        recent_start = max(0, seq_len - self.query_history_size)\n        recent_keys = keys[:, :, recent_start:, :].float()\n        \n        self.query_mean = recent_keys.mean(dim=2)\n        self.query_cov_diag = recent_keys.var(dim=2) + 1e-8\n\n        keys_float = keys.float()\n        mu_term = torch.einsum('bhd,bhsd->bhs', self.query_mean, keys_float) / (head_dim ** 0.5)\n        quad_term = ((keys_float ** 2) * self.query_cov_diag.unsqueeze(2)).sum(dim=-1) / (2 * head_dim)\n        \n        log_importance = (mu_term + quad_term) / self.temperature\n        importance = torch.softmax(log_importance, dim=-1)\n        \n        return importance.to(dtype)\n\n\nclass TRIMKVScorer:\n    def __init__(self, decay_base=0.95):\n        self.decay_base = decay_base\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n\n        key_norms = keys.float().norm(dim=-1)\n        base_importance = key_norms / (key_norms.max(dim=-1, keepdim=True)[0] + 1e-8)\n\n        positions = torch.arange(seq_len, device=device).float()\n        relative_age = (seq_len - 1 - positions) / max(seq_len, 1)\n        decay = self.decay_base ** relative_age\n        decay = decay.unsqueeze(0).unsqueeze(0)\n\n        importance = base_importance * decay\n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n\n        return importance.to(dtype)\n\n\nclass WriteGatedKVScorer:\n    def __init__(self, attention_weight=0.3):\n        self.attention_weight = attention_weight\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n\n        key_norms = keys.float().norm(dim=-1)\n        value_norms = values.float().norm(dim=-1)\n        \n        kv_importance = (key_norms + value_norms) / 2\n        kv_importance = kv_importance / (kv_importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n\n        if attentions is not None:\n            if attentions.dim() == 4:\n                attn = attentions[:, :, -1, :]\n            else:\n                attn = attentions\n            \n            if attn.shape[-1] >= seq_len:\n                attn = attn[:, :, :seq_len]\n            else:\n                attn = F.pad(attn, (0, seq_len - attn.shape[-1]))\n            \n            attn_norm = attn / (attn.max(dim=-1, keepdim=True)[0] + 1e-8)\n            importance = (1 - self.attention_weight) * kv_importance + self.attention_weight * attn_norm.to(kv_importance.dtype)\n        else:\n            importance = kv_importance\n\n        return importance.to(dtype)\n\n\ndef create_sip_importance_scorer(sip_model):\n    class SIPWrapper:\n        def __init__(self, model):\n            self.model = model\n            self.model.eval()\n\n        def reset(self):\n            pass\n\n        def __call__(self, keys, values, attentions):\n            with torch.no_grad():\n                positions = torch.arange(keys.shape[2], device=keys.device).unsqueeze(0)\n                importance = self.model(keys, values, positions)\n            return importance\n\n    return SIPWrapper(sip_model)\n\n\nSCORER_REGISTRY = {\n    'h2o': H2OImportanceScorer,\n    'snapkv': SnapKVImportanceScorer,\n    'streamingllm': StreamingLLMScorer,\n    'recent': RecentImportanceScorer,\n    'random': RandomImportanceScorer,\n    'expected_attention': ExpectedAttentionScorer,\n    'trimkv': TRIMKVScorer,\n    'write_gated': WriteGatedKVScorer,\n}\n\ndef get_scorer(name, **kwargs):\n    if name not in SCORER_REGISTRY:\n        raise ValueError(f\"Unknown scorer: {name}. Available: {list(SCORER_REGISTRY.keys())}\")\n    return SCORER_REGISTRY[name](**kwargs)\n\n\ncompression_config = CompressionConfig(\n    attention_sink_size=4,\n    recent_window_size=32,\n    min_cache_size=64,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vd998yn2goj",
   "metadata": {},
   "outputs": [],
   "source": "class PyramidKVScorer:\n    def __init__(self, num_layers=22, base_ratio=0.5):\n        self.num_layers = num_layers\n        self.base_ratio = base_ratio\n        self.layer_ratios = self._compute_pyramid_ratios()\n    \n    def _compute_pyramid_ratios(self):\n        ratios = []\n        for i in range(self.num_layers):\n            layer_ratio = 0.3 + 0.4 * (i / max(1, self.num_layers - 1))\n            ratios.append(layer_ratio * self.base_ratio)\n        return ratios\n    \n    def reset(self):\n        pass\n    \n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n        \n        if attentions is not None:\n            if attentions.dim() == 4:\n                importance = attentions[:, :, -1, :seq_len]\n            else:\n                importance = attentions[:, :, :seq_len]\n        else:\n            importance = torch.ones(batch, heads, seq_len, device=device)\n            recency = torch.linspace(0.5, 1.0, seq_len, device=device)\n            importance = importance * recency.unsqueeze(0).unsqueeze(0)\n        \n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance.to(dtype)\n\n\nclass AdaKVScorer:\n    def __init__(self, min_ratio=0.25, max_ratio=0.9):\n        self.min_ratio = min_ratio\n        self.max_ratio = max_ratio\n    \n    def reset(self):\n        pass\n    \n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n        \n        if attentions is not None:\n            if attentions.dim() == 4:\n                attn = attentions[:, :, -1, :seq_len]\n            else:\n                attn = attentions[:, :, :seq_len]\n            \n            attn_probs = attn / (attn.sum(dim=-1, keepdim=True) + 1e-8)\n            entropy = -(attn_probs * (attn_probs + 1e-8).log()).sum(dim=-1)\n            max_entropy = np.log(seq_len)\n            normalized_entropy = entropy / max_entropy\n            \n            difficulty = normalized_entropy.mean()\n            adaptive_weight = self.min_ratio + (self.max_ratio - self.min_ratio) * difficulty\n            \n            importance = attn * adaptive_weight\n        else:\n            importance = torch.ones(batch, heads, seq_len, device=device)\n        \n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance.to(dtype)\n\n\nclass DMCScorer:\n    def __init__(self, num_heads=32, num_kv_heads=4):\n        self.num_heads = num_heads\n        self.num_kv_heads = num_kv_heads\n        self.head_weights = torch.ones(num_kv_heads)\n    \n    def reset(self):\n        pass\n    \n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n        \n        key_norms = keys.float().norm(dim=-1)\n        value_norms = values.float().norm(dim=-1)\n        \n        head_weights = self.head_weights.to(device).unsqueeze(0).unsqueeze(-1)\n        importance = (key_norms + value_norms) / 2 * head_weights\n        \n        if attentions is not None:\n            if attentions.dim() == 4:\n                attn = attentions[:, :, -1, :seq_len]\n            else:\n                attn = attentions[:, :, :seq_len]\n            importance = 0.5 * importance + 0.5 * attn.to(importance.dtype)\n        \n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance.to(dtype)\n\n\nclass KIVIScorer:\n    def __init__(self):\n        pass\n    \n    def reset(self):\n        pass\n    \n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n        \n        keys_float = keys.float()\n        values_float = values.float()\n        \n        key_var = keys_float.var(dim=-1)\n        value_var = values_float.var(dim=-1)\n        \n        key_mag = keys_float.norm(dim=-1)\n        value_mag = values_float.norm(dim=-1)\n        \n        importance = (key_var + value_var) * (key_mag + value_mag)\n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n        \n        return importance.to(dtype)\n\n\nclass TOVAScorer:\n    def __init__(self, window_size=64):\n        self.window_size = window_size\n        self.attention_history = None\n    \n    def reset(self):\n        self.attention_history = None\n    \n    def __call__(self, keys, values, attentions):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        dtype = keys.dtype\n        \n        if attentions is not None:\n            if attentions.dim() == 4:\n                query_len = attentions.shape[2]\n                window_start = max(0, query_len - self.window_size)\n                attn_window = attentions[:, :, window_start:, :seq_len]\n                importance = attn_window.sum(dim=2)\n            else:\n                importance = attentions[:, :, :seq_len]\n            \n            if self.attention_history is None:\n                self.attention_history = importance\n            else:\n                if self.attention_history.shape[-1] < seq_len:\n                    pad = seq_len - self.attention_history.shape[-1]\n                    self.attention_history = F.pad(self.attention_history, (0, pad))\n                self.attention_history = 0.9 * self.attention_history + 0.1 * importance\n                importance = self.attention_history\n        else:\n            importance = torch.ones(batch, heads, seq_len, device=device)\n        \n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance.to(dtype)\n\n\nSCORER_REGISTRY.update({\n    'pyramidkv': PyramidKVScorer,\n    'adakv': AdaKVScorer,\n    'dmc': DMCScorer,\n    'kivi': KIVIScorer,\n    'tova': TOVAScorer,\n})"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Dataset with GQA Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "class RolloutDataset(Dataset):\n    def __init__(self, texts, model, tokenizer, config, max_seq_length=512, rollout_length=32, device='cuda', cache_dir=None):\n        self.config = config\n        self.device = device\n        self.cache_dir = cache_dir\n        \n        self.num_attention_heads = model.config.num_attention_heads\n        self.num_kv_heads = getattr(model.config, 'num_key_value_heads', model.config.num_attention_heads)\n        self.head_group_size = self.num_attention_heads // self.num_kv_heads\n\n        self.data = []\n        \n        if cache_dir and os.path.exists(os.path.join(cache_dir, 'rollout_data.pt')):\n            self.data = torch.load(os.path.join(cache_dir, 'rollout_data.pt'))\n            print(f\"Loaded {len(self.data)} cached samples\")\n            return\n        \n        for text in tqdm(texts, desc=\"Generating rollouts\"):\n            tokens = tokenizer(text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n            if tokens.input_ids.shape[1] < 128:\n                continue\n            \n            item = self._generate_rollout(tokens.input_ids.squeeze(0), model, rollout_length)\n            if item is not None:\n                self.data.append(item)\n        \n        print(f\"Dataset: {len(self.data)} sequences\")\n        \n        if cache_dir:\n            os.makedirs(cache_dir, exist_ok=True)\n            torch.save(self.data, os.path.join(cache_dir, 'rollout_data.pt'))\n\n    def __len__(self):\n        return len(self.data)\n\n    def _group_attention_heads(self, attn):\n        if self.head_group_size == 1:\n            return attn\n        if attn.dim() == 2:\n            h, s = attn.shape\n            return attn.view(self.num_kv_heads, self.head_group_size, s).mean(dim=1)\n        else:\n            b, h, s = attn.shape\n            return attn.view(b, self.num_kv_heads, self.head_group_size, s).mean(dim=2)\n\n    @torch.no_grad()\n    def _generate_rollout(self, input_ids, model, rollout_length):\n        try:\n            input_ids = input_ids.unsqueeze(0).to(self.device)\n            outputs = model(input_ids, use_cache=True, output_attentions=True, return_dict=True)\n            past_kv = outputs.past_key_values\n            initial_seq_len = input_ids.shape[1]\n\n            all_future_attention = []\n            for step in range(min(rollout_length, self.config.lookahead_steps * 2)):\n                next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n                outputs = model(next_token, past_key_values=past_kv, use_cache=True, output_attentions=True, return_dict=True)\n                past_kv = outputs.past_key_values\n                layer_attns = [a.squeeze(2) for a in outputs.attentions]\n                avg_attn = torch.stack(layer_attns).mean(dim=0).squeeze(0)\n                grouped = self._group_attention_heads(avg_attn)\n                all_future_attention.append(grouped)\n\n            if all_future_attention:\n                max_seq = max(a.shape[-1] for a in all_future_attention)\n                padded = [F.pad(a, (0, max_seq - a.shape[-1])) for a in all_future_attention]\n                future_attention = torch.stack(padded, dim=1)[:, :, :initial_seq_len]\n            else:\n                future_attention = torch.zeros(self.num_kv_heads, 1, initial_seq_len, device=self.device)\n\n            keys = past_kv[0][0][:, :, :initial_seq_len, :].squeeze(0)\n            values = past_kv[0][1][:, :, :initial_seq_len, :].squeeze(0)\n            positions = torch.arange(initial_seq_len, device=self.device)\n\n            return {\n                'keys': keys.cpu(),\n                'values': values.cpu(),\n                'positions': positions.cpu(),\n                'future_attention': future_attention.cpu(),\n            }\n        except Exception as e:\n            return None\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\ndef collate_fn(batch):\n    max_seq = max(item['keys'].shape[1] for item in batch)\n    max_rollout = max(item['future_attention'].shape[1] for item in batch)\n    keys_list, values_list, positions_list, future_attention_list, masks = [], [], [], [], []\n    for item in batch:\n        seq_len = item['keys'].shape[1]\n        rollout_len = item['future_attention'].shape[1]\n        keys_list.append(F.pad(item['keys'], (0, 0, 0, max_seq - seq_len)))\n        values_list.append(F.pad(item['values'], (0, 0, 0, max_seq - seq_len)))\n        positions_list.append(F.pad(item['positions'], (0, max_seq - seq_len)))\n        future_attention_list.append(F.pad(item['future_attention'], (0, max_seq - seq_len, 0, max_rollout - rollout_len)))\n        mask = torch.zeros(max_seq)\n        mask[:seq_len] = 1.0\n        masks.append(mask)\n    return {\n        'keys': torch.stack(keys_list),\n        'values': torch.stack(values_list),\n        'positions': torch.stack(positions_list),\n        'future_attention': torch.stack(future_attention_list),\n        'mask': torch.stack(masks),\n    }"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Curriculum Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "class CurriculumScheduler:\n    def __init__(self, initial_lookahead=1, max_lookahead=8, warmup_epochs=3):\n        self.initial = initial_lookahead\n        self.max = max_lookahead\n        self.warmup = warmup_epochs\n\n    def get_lookahead(self, epoch):\n        if epoch < self.warmup:\n            return self.initial\n        progress = min(1.0, (epoch - self.warmup) / max(1, self.warmup * 2))\n        return min(self.max, self.initial + int(progress * (self.max - self.initial)))\n\n\nclass TemperatureScaling(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.temperature = nn.Parameter(torch.ones(1))\n\n    def forward(self, logits):\n        return logits / self.temperature\n\n    @torch.no_grad()\n    def fit(self, confidences, accuracies, num_iters=50):\n        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=num_iters)\n        def closure():\n            optimizer.zero_grad()\n            scaled = torch.sigmoid(confidences / self.temperature)\n            loss = F.binary_cross_entropy(scaled, accuracies)\n            loss.backward()\n            return loss\n        optimizer.step(closure)\n        return self.temperature.item()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading TinyLlama...\")\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    attn_implementation=\"eager\",\n)\nmodel.eval()\ntokenizer.pad_token = tokenizer.eos_token"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\ntrain_texts = [t for t in dataset[\"train\"][\"text\"] if len(t.strip()) > 200][:TRAINING_CONFIG[\"num_train_samples\"]]\nval_texts = [t for t in dataset[\"validation\"][\"text\"] if len(t.strip()) > 200][:TRAINING_CONFIG[\"num_val_samples\"]]\ntest_texts = [t for t in dataset[\"test\"][\"text\"] if len(t.strip()) > 200][:200]\nprint(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "sip_config = SIPConfig(\n    hidden_dim=model.config.hidden_size,\n    num_heads=model.config.num_attention_heads,\n    num_kv_heads=getattr(model.config, 'num_key_value_heads', model.config.num_attention_heads),\n    head_dim=model.config.hidden_size // model.config.num_attention_heads,\n    lookahead_steps=TRAINING_CONFIG[\"max_lookahead\"],\n)\n\nsip_scorer = SpeculativeImportanceScorer(sip_config).to(device)\nprint(f\"SIP Parameters: {sum(p.numel() for p in sip_scorer.parameters()):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "train_dataset = RolloutDataset(\n    train_texts, model, tokenizer, sip_config, \n    device=device, \n    cache_dir='./cache/train'\n)\nval_dataset = RolloutDataset(\n    val_texts, model, tokenizer, sip_config, \n    device=device,\n    cache_dir='./cache/val'\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAINING_CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=TRAINING_CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Training with Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "loss_fn = ImprovedSIPLoss(\n    kl_weight=TRAINING_CONFIG[\"kl_weight\"],\n    ranking_weight=TRAINING_CONFIG[\"ranking_weight\"],\n    confidence_weight=TRAINING_CONFIG[\"confidence_weight\"],\n    temporal_weight=TRAINING_CONFIG[\"temporal_weight\"],\n)\n\noptimizer = AdamW(sip_scorer.parameters(), lr=TRAINING_CONFIG[\"lr\"], weight_decay=TRAINING_CONFIG[\"weight_decay\"])\ntotal_steps = len(train_loader) * TRAINING_CONFIG[\"epochs\"]\nwarmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=TRAINING_CONFIG[\"warmup_steps\"])\nmain_scheduler = CosineAnnealingLR(optimizer, T_max=total_steps - TRAINING_CONFIG[\"warmup_steps\"])\nscheduler = SequentialLR(optimizer, [warmup_scheduler, main_scheduler], milestones=[TRAINING_CONFIG[\"warmup_steps\"]])\n\ncurriculum = CurriculumScheduler(\n    initial_lookahead=TRAINING_CONFIG[\"initial_lookahead\"],\n    max_lookahead=TRAINING_CONFIG[\"max_lookahead\"],\n    warmup_epochs=TRAINING_CONFIG[\"curriculum_warmup\"],\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "best_val_loss = float('inf')\nhistory = {'train_loss': [], 'val_loss': [], 'val_corr': [], 'lookahead': []}\naccumulation_steps = TRAINING_CONFIG[\"accumulation_steps\"]\n\nfor epoch in range(TRAINING_CONFIG[\"epochs\"]):\n    current_lookahead = curriculum.get_lookahead(epoch)\n    history['lookahead'].append(current_lookahead)\n    \n    print(f\"Epoch {epoch+1}/{TRAINING_CONFIG['epochs']} | Lookahead: {current_lookahead}\")\n\n    sip_scorer.train()\n    train_losses = []\n    optimizer.zero_grad()\n    \n    pbar = tqdm(train_loader, desc=\"Training\")\n    for batch_idx, batch in enumerate(pbar):\n        keys = batch['keys'].to(device).float()\n        values = batch['values'].to(device).float()\n        positions = batch['positions'].to(device)\n        future_attention = batch['future_attention'].to(device).float()\n\n        kv_features = sip_scorer.kv_encoder(keys, values)\n        pred, conf = sip_scorer.predictor(kv_features, positions, keys.shape[2], current_lookahead)\n        \n        losses = loss_fn(pred.float(), conf.float(), future_attention[:, :, :current_lookahead, :])\n        loss = losses['total'] / accumulation_steps\n        loss.backward()\n\n        if (batch_idx + 1) % accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(sip_scorer.parameters(), 1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        train_losses.append(losses['total'].item())\n        pbar.set_postfix({'loss': f\"{losses['total'].item():.4f}\"})\n\n    sip_scorer.eval()\n    val_losses, val_corrs = [], []\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation\"):\n            keys = batch['keys'].to(device).float()\n            values = batch['values'].to(device).float()\n            positions = batch['positions'].to(device)\n            future_attention = batch['future_attention'].to(device).float()\n\n            kv_features = sip_scorer.kv_encoder(keys, values)\n            pred, conf = sip_scorer.predictor(kv_features, positions, keys.shape[2], current_lookahead)\n            losses = loss_fn(pred.float(), conf.float(), future_attention[:, :, :current_lookahead, :])\n            val_losses.append(losses['total'].item())\n\n            p = pred[:, :, :, 0].flatten()\n            t = future_attention[:, :, 0, :]\n            t = (t / (t.sum(dim=-1, keepdim=True) + 1e-8)).flatten()\n            if p.numel() > 1:\n                c = torch.corrcoef(torch.stack([p, t]))[0, 1]\n                if not torch.isnan(c):\n                    val_corrs.append(c.item())\n\n    avg_train = np.mean(train_losses)\n    avg_val = np.mean(val_losses)\n    avg_corr = np.mean(val_corrs) if val_corrs else 0\n    \n    history['train_loss'].append(avg_train)\n    history['val_loss'].append(avg_val)\n    history['val_corr'].append(avg_corr)\n\n    print(f\"Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f} | Correlation: {avg_corr:.4f}\")\n\n    if avg_val < best_val_loss:\n        best_val_loss = avg_val\n        torch.save(sip_scorer.state_dict(), 'best_sip_improved.pt')\n        print(\"Saved best model\")\n\nprint(f\"Training complete | Best Val Loss: {best_val_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Evaluation WITH Actual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "sip_scorer.load_state_dict(torch.load('best_sip_improved.pt', weights_only=True))\nsip_scorer.eval()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "sip_scorer.load_state_dict(torch.load('best_sip_improved.pt', weights_only=True))\nsip_scorer.eval()\n\n\nclass PrefillAttentionScorer:\n    def __init__(self):\n        self.prefill_attention = None\n    \n    def reset(self):\n        self.prefill_attention = None\n    \n    def __call__(self, keys, values, prefill_attention=None):\n        if prefill_attention is not None:\n            if prefill_attention.dim() == 4:\n                self.prefill_attention = prefill_attention.mean(dim=2)\n            else:\n                self.prefill_attention = prefill_attention\n        return self.prefill_attention\n\n\nclass PositionHeuristicScorer:\n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, _ = keys.shape\n        importance = torch.ones(batch, heads, seq_len, device=keys.device)\n        importance[:, :, :4] = 10.0\n        importance[:, :, -32:] = 5.0\n        return importance\n\n\nclass SIPPreGenerationScorer:\n    def __init__(self, sip_model):\n        self.model = sip_model\n        self.model.eval()\n    \n    def reset(self):\n        pass\n    \n    def __call__(self, keys, values, attention=None):\n        with torch.no_grad():\n            positions = torch.arange(keys.shape[2], device=keys.device).unsqueeze(0)\n            return self.model(keys.float(), values.float(), positions)\n\n\nclass ExpectedAttentionPreGenScorer:\n    def __init__(self):\n        self.scorer = ExpectedAttentionScorer()\n    \n    def reset(self):\n        self.scorer.reset()\n    \n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, _ = keys.shape\n        dummy_attn = torch.ones(batch, heads, seq_len, device=keys.device) / seq_len\n        return self.scorer(keys, values, dummy_attn)\n\n\nclass TRIMKVPreGenScorer:\n    def __init__(self):\n        self.scorer = TRIMKVScorer()\n    \n    def reset(self):\n        self.scorer.reset()\n    \n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, _ = keys.shape\n        dummy_attn = torch.ones(batch, heads, seq_len, device=keys.device) / seq_len\n        return self.scorer(keys, values, dummy_attn)\n\n\nclass WriteGatedPreGenScorer:\n    def __init__(self):\n        self.scorer = WriteGatedKVScorer()\n    \n    def reset(self):\n        self.scorer.reset()\n    \n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, _ = keys.shape\n        dummy_attn = torch.ones(batch, heads, seq_len, device=keys.device) / seq_len\n        return self.scorer(keys, values, dummy_attn)\n\n@torch.no_grad()\ndef compute_perplexity_with_compression_fixed(\n    model, tokenizer, texts, scorer, scorer_name,\n    retention_ratio=0.5, min_cache=48, max_samples=50, device='cuda',\n):\n    model.eval()\n    \n    num_attention_heads = model.config.num_attention_heads\n    num_kv_heads = getattr(model.config, 'num_key_value_heads', num_attention_heads)\n    head_group_size = num_attention_heads // num_kv_heads\n\n    def group_attention_heads(attn):\n        if head_group_size == 1:\n            return attn\n        batch, heads, *rest = attn.shape\n        return attn.view(batch, num_kv_heads, head_group_size, *rest).mean(dim=2)\n\n    total_nll = 0.0\n    total_tokens = 0\n\n    for text in tqdm(texts[:max_samples], desc=f\"{scorer_name} @ {retention_ratio:.0%}\"):\n        if hasattr(scorer, 'reset'):\n            scorer.reset()\n\n        tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n        seq_len = tokens.shape[1]\n        if seq_len < 128:\n            continue\n\n        split_point = seq_len * 3 // 4\n        context_ids = tokens[:, :split_point]\n        continuation_ids = tokens[:, split_point:]\n        continuation_len = continuation_ids.shape[1]\n\n        outputs = model(context_ids, use_cache=True, output_attentions=True, return_dict=True)\n        past_kv = outputs.past_key_values\n        \n        keys, values = get_cache_keys_values(past_kv, 0)\n        prefill_attention = outputs.attentions[-1]\n        prefill_attention_grouped = group_attention_heads(prefill_attention)\n\n        if isinstance(scorer, PrefillAttentionScorer):\n            importance = scorer(keys, values, prefill_attention_grouped)\n        else:\n            importance = scorer(keys, values, None)\n\n        if importance.dim() == 3:\n            avg_importance = importance.mean(dim=(0, 1))\n        elif importance.dim() == 2:\n            avg_importance = importance.mean(dim=0)\n        else:\n            avg_importance = importance.flatten()\n        \n        budget = max(int(split_point * retention_ratio), min_cache)\n        budget = min(budget, split_point)\n        \n        keep_mask = torch.zeros(split_point, dtype=torch.bool, device=device)\n        sink_size = 4\n        recent_size = min(16, split_point - sink_size)\n        keep_mask[:sink_size] = True\n        keep_mask[-recent_size:] = True\n        \n        reserved = keep_mask.sum().item()\n        remaining_budget = max(0, budget - reserved)\n        \n        if remaining_budget > 0:\n            importance_masked = avg_importance.clone()\n            importance_masked[keep_mask] = -float('inf')\n            k = min(remaining_budget, (~keep_mask).sum().item())\n            if k > 0:\n                _, top_indices = importance_masked.topk(k)\n                keep_mask[top_indices] = True\n        \n        keep_indices = keep_mask.nonzero(as_tuple=True)[0].sort()[0]\n        \n        compressed_kv = DynamicCache()\n        for layer_idx in range(get_cache_length(past_kv)):\n            layer_keys, layer_values = get_cache_keys_values(past_kv, layer_idx)\n            compressed_kv.update(\n                layer_keys.index_select(2, keep_indices),\n                layer_values.index_select(2, keep_indices),\n                layer_idx\n            )\n\n        current_kv = compressed_kv\n        current_pos = split_point\n        \n        for i in range(continuation_len - 1):\n            input_token = continuation_ids[:, i:i+1]\n            target_token = continuation_ids[:, i+1]\n            position_ids = torch.tensor([[current_pos]], device=device)\n            \n            out = model(\n                input_token,\n                past_key_values=current_kv,\n                position_ids=position_ids,\n                use_cache=True,\n                return_dict=True,\n            )\n            \n            logits = out.logits[:, -1, :]\n            loss = F.cross_entropy(logits, target_token, reduction='sum')\n            \n            if not torch.isnan(loss):\n                total_nll += loss.item()\n                total_tokens += 1\n            \n            current_kv = out.past_key_values\n            current_pos += 1\n\n    if total_tokens == 0:\n        return {'perplexity': float('inf'), 'total_tokens': 0}\n\n    perplexity = np.exp(total_nll / total_tokens)\n    return {'perplexity': perplexity, 'total_tokens': total_tokens}\n\n\npregeneration_scorers = {\n    'SIP (Ours)': SIPPreGenerationScorer(sip_scorer),\n    'Expected-Attn': ExpectedAttentionPreGenScorer(),\n    'TRIM-KV': TRIMKVPreGenScorer(),                  \n    'Write-Gated': WriteGatedPreGenScorer(),           \n    'Prefill-Attn': PrefillAttentionScorer(),\n    'Position-Heuristic': PositionHeuristicScorer(),\n    'Recent-Only': RecentImportanceScorer(),\n    'Random': RandomImportanceScorer(),\n}\n\nretention_ratios = [0.10, 0.25, 0.50, 0.75]\npregeneration_results = {}\n\nprint(\"Baseline (Full Cache)\")\ntotal_nll = 0\ntotal_tokens = 0\nfor text in tqdm(test_texts[:50], desc=\"Baseline\"):\n    tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n    if tokens.shape[1] < 128:\n        continue\n    split_point = tokens.shape[1] * 3 // 4\n    context = tokens[:, :split_point]\n    continuation = tokens[:, split_point:]\n    \n    with torch.no_grad():\n        outputs = model(context, use_cache=True, return_dict=True)\n        current_kv = outputs.past_key_values\n        current_pos = split_point\n        \n        for i in range(continuation.shape[1] - 1):\n            input_token = continuation[:, i:i+1]\n            target_token = continuation[:, i+1]\n            position_ids = torch.tensor([[current_pos]], device=device)\n            \n            out = model(input_token, past_key_values=current_kv, position_ids=position_ids, \n                       use_cache=True, return_dict=True)\n            logits = out.logits[:, -1, :]\n            loss = F.cross_entropy(logits, target_token, reduction='sum')\n            \n            if not torch.isnan(loss):\n                total_nll += loss.item()\n                total_tokens += 1\n            \n            current_kv = out.past_key_values\n            current_pos += 1\n\nbaseline_ppl = np.exp(total_nll / total_tokens)\npregeneration_results['Full Cache'] = {1.0: baseline_ppl}\nprint(f\"Baseline Perplexity: {baseline_ppl:.2f}\")\n\nfor scorer_name, scorer in pregeneration_scorers.items():\n    pregeneration_results[scorer_name] = {}\n    \n    for ratio in retention_ratios:\n        result = compute_perplexity_with_compression_fixed(\n            model=model,\n            tokenizer=tokenizer,\n            texts=test_texts,\n            scorer=scorer,\n            scorer_name=scorer_name,\n            retention_ratio=ratio,\n            min_cache=48,\n            max_samples=50,\n            device=str(device),\n        )\n        \n        pregeneration_results[scorer_name][ratio] = result['perplexity']\n        print(f\"{scorer_name} @ {ratio:.0%}: {result['perplexity']:.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Baseline (Full Cache): {baseline_ppl:.2f}\")\n\nprint(\"\\nPerplexity by Method\")\nheader = f\"{'Method':<20} |\"\nfor ratio in retention_ratios:\n    header += f\" {int(ratio*100):>6}% |\"\nprint(header)\nprint(\"-\"*70)\n\nmethod_order = ['SIP (Ours)', 'Expected-Attn', 'TRIM-KV', 'Write-Gated', \n                'Prefill-Attn', 'Position-Heuristic', 'Recent-Only', 'Random']\n\nfor method in method_order:\n    if method in pregeneration_results:\n        row = f\"{method:<20} |\"\n        for ratio in retention_ratios:\n            if ratio in pregeneration_results[method]:\n                ppl = pregeneration_results[method][ratio]\n                row += f\" {ppl:>7.2f} |\"\n            else:\n                row += f\" {'--':>7} |\"\n        print(row)\n\nprint(\"\\nDegradation from Full Cache (% increase in PPL)\")\nheader = f\"{'Method':<20} |\"\nfor ratio in retention_ratios:\n    header += f\" {int(ratio*100):>6}% |\"\nprint(header)\nprint(\"-\"*70)\n\nfor method in method_order:\n    if method in pregeneration_results:\n        row = f\"{method:<20} |\"\n        for ratio in retention_ratios:\n            if ratio in pregeneration_results[method]:\n                ppl = pregeneration_results[method][ratio]\n                degradation = ((ppl - baseline_ppl) / baseline_ppl) * 100\n                row += f\" {degradation:>+6.1f}% |\"\n            else:\n                row += f\" {'--':>7} |\"\n        print(row)\n\nprint(\"\\nBest Method at Each Retention Level\")\nfor ratio in retention_ratios:\n    best_method = None\n    best_ppl = float('inf')\n    for method in method_order:\n        if method in pregeneration_results and ratio in pregeneration_results[method]:\n            ppl = pregeneration_results[method][ratio]\n            if ppl < best_ppl:\n                best_ppl = ppl\n                best_method = method\n    if best_method:\n        degradation = ((best_ppl - baseline_ppl) / baseline_ppl) * 100\n        print(f\"  {int(ratio*100)}%: {best_method} (PPL: {best_ppl:.2f}, {degradation:+.1f}% from baseline)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9v377j1c0u",
   "metadata": {},
   "outputs": [],
   "source": "def run_multiseed_evaluation(\n    model, tokenizer, texts, scorers, retention_ratios, \n    num_seeds=5, max_samples=30, device='cuda',\n):\n    all_results = {name: {r: [] for r in retention_ratios} for name in scorers.keys()}\n\n    for seed in range(num_seeds):\n        print(f\"Seed {seed + 1}/{num_seeds}\")\n        torch.manual_seed(42 + seed)\n        np.random.seed(42 + seed)\n\n        shuffled_texts = texts.copy()\n        np.random.shuffle(shuffled_texts)\n\n        for scorer_name, scorer in scorers.items():\n            for ratio in retention_ratios:\n                if hasattr(scorer, 'reset'):\n                    scorer.reset()\n\n                result = compute_perplexity_with_compression_fixed(\n                    model=model,\n                    tokenizer=tokenizer,\n                    texts=shuffled_texts,\n                    scorer=scorer,\n                    scorer_name=f\"{scorer_name} (seed {seed})\",\n                    retention_ratio=ratio,\n                    min_cache=48,\n                    max_samples=max_samples,\n                    device=device,\n                )\n\n                all_results[scorer_name][ratio].append(result['perplexity'])\n\n    stats_results = {}\n    for scorer_name in scorers.keys():\n        stats_results[scorer_name] = {}\n        for ratio in retention_ratios:\n            values = all_results[scorer_name][ratio]\n            mean = np.mean(values)\n            std = np.std(values, ddof=1)\n            n = len(values)\n            ci_95 = 1.96 * std / np.sqrt(n)\n\n            stats_results[scorer_name][ratio] = {\n                'mean': mean,\n                'std': std,\n                'ci_95': ci_95,\n                'values': values,\n            }\n\n    return stats_results\n\n\ndef compute_significance(results, baseline_name, method_name, ratio):\n    baseline_values = results[baseline_name][ratio]['values']\n    method_values = results[method_name][ratio]['values']\n\n    if len(baseline_values) < 2 or len(method_values) < 2:\n        return None\n\n    t_stat, p_value = stats.ttest_rel(baseline_values, method_values)\n    return {'t_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05}\n\n\nmultiseed_scorers = {\n    'SIP (Ours)': SIPPreGenerationScorer(sip_scorer),\n    'Prefill-Attn': PrefillAttentionScorer(),\n    'Position-Heuristic': PositionHeuristicScorer(),\n    'Expected-Attn': ExpectedAttentionPreGenScorer(),\n    'TRIM-KV': TRIMKVPreGenScorer(),\n    'Random': RandomImportanceScorer(),\n}\n\nmultiseed_ratios = [0.10, 0.25, 0.50, 0.75]\nNUM_SEEDS = 5\n\nprint(f\"Running {NUM_SEEDS}-seed evaluation across {len(multiseed_scorers)} methods...\")\n\nmultiseed_results = run_multiseed_evaluation(\n    model=model,\n    tokenizer=tokenizer,\n    texts=test_texts,\n    scorers=multiseed_scorers,\n    retention_ratios=multiseed_ratios,\n    num_seeds=NUM_SEEDS,\n    max_samples=30,\n    device=str(device),\n)\n\nprint(\"\\nMulti-Seed Results (Mean +/- 95% CI)\")\n\nheader = f\"{'Method':<20} |\"\nfor ratio in multiseed_ratios:\n    header += f\" {int(ratio*100):>12}% |\"\nprint(header)\nprint(\"-\"*75)\n\nmethod_order = ['SIP (Ours)', 'Prefill-Attn', 'Position-Heuristic', 'Expected-Attn', 'TRIM-KV', 'Random']\nfor method in method_order:\n    if method in multiseed_results:\n        row = f\"{method:<20} |\"\n        for ratio in multiseed_ratios:\n            if ratio in multiseed_results[method]:\n                stats_data = multiseed_results[method][ratio]\n                row += f\" {stats_data['mean']:.2f}+/-{stats_data['ci_95']:.2f} |\"\n            else:\n                row += f\" {'--':>12} |\"\n        print(row)\n\nprint(\"\\nStatistical Significance (vs Random baseline)\")\n\nfor method in ['SIP (Ours)', 'Prefill-Attn', 'Position-Heuristic', 'Expected-Attn', 'TRIM-KV']:\n    if method in multiseed_results:\n        row = f\"{method:<20} |\"\n        for ratio in multiseed_ratios:\n            sig = compute_significance(multiseed_results, 'Random', method, ratio)\n            if sig:\n                if sig['p_value'] < 0.001:\n                    marker = \"***\"\n                elif sig['p_value'] < 0.01:\n                    marker = \"**\"\n                elif sig['p_value'] < 0.05:\n                    marker = \"*\"\n                else:\n                    marker = \"ns\"\n                row += f\" {marker:>12} |\"\n            else:\n                row += f\" {'--':>12} |\"\n        print(row)\n\nprint(\"\\nSIP vs Prefill-Attn:\")\nfor ratio in multiseed_ratios:\n    sig = compute_significance(multiseed_results, 'Prefill-Attn', 'SIP (Ours)', ratio)\n    if sig:\n        sip_mean = multiseed_results['SIP (Ours)'][ratio]['mean']\n        pfa_mean = multiseed_results['Prefill-Attn'][ratio]['mean']\n        diff = sip_mean - pfa_mean\n        sig_str = f\"p={sig['p_value']:.4f}\" + (\" *\" if sig['significant'] else \" ns\")\n        print(f\"  {int(ratio*100):>3}%: SIP {sip_mean:.2f} vs Prefill-Attn {pfa_mean:.2f} (d={diff:+.2f}) {sig_str}\")\n\nprint(\"\\nSIP vs Position-Heuristic:\")\nfor ratio in multiseed_ratios:\n    sig = compute_significance(multiseed_results, 'Position-Heuristic', 'SIP (Ours)', ratio)\n    if sig:\n        sip_mean = multiseed_results['SIP (Ours)'][ratio]['mean']\n        pos_mean = multiseed_results['Position-Heuristic'][ratio]['mean']\n        diff = sip_mean - pos_mean\n        sig_str = f\"p={sig['p_value']:.4f}\" + (\" *\" if sig['significant'] else \" ns\")\n        print(f\"  {int(ratio*100):>3}%: SIP {sip_mean:.2f} vs Position-Heuristic {pos_mean:.2f} (d={diff:+.2f}) {sig_str}\")\n\nprint(\"\\nBest method at each retention level (by mean PPL):\")\nfor ratio in multiseed_ratios:\n    best_method = None\n    best_mean = float('inf')\n    for method in method_order:\n        if method in multiseed_results and ratio in multiseed_results[method]:\n            mean = multiseed_results[method][ratio]['mean']\n            if mean < best_mean:\n                best_mean = mean\n                best_method = method\n    if best_method:\n        ci = multiseed_results[best_method][ratio]['ci_95']\n        print(f\"  {int(ratio*100):>3}%: {best_method} ({best_mean:.2f} +/- {ci:.2f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "6ht0ju76dao",
   "metadata": {},
   "source": [
    "## 9.5. Task-Based Evaluation (QA Accuracy)\n",
    "\n",
    "This evaluates how well the model can answer questions AFTER KV cache compression.\n",
    "More practical than perplexity - measures actual task performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4gov2hr1kee",
   "metadata": {},
   "outputs": [],
   "source": "qa_examples = [\n    {\"context\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world. The tower is 330 metres (1,083 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris.\",\n     \"question\": \"Who designed the Eiffel Tower?\", \"answer\": \"Gustave Eiffel\", \"answer_variations\": [\"gustave eiffel\", \"eiffel\", \"gustave\"]},\n    {\"context\": \"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, and Venezuela.\",\n     \"question\": \"Which country contains the majority of the Amazon rainforest?\", \"answer\": \"Brazil\", \"answer_variations\": [\"brazil\", \"brasil\"]},\n    {\"context\": \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time. Einstein is best known for developing the theory of relativity, but he also made important contributions to quantum mechanics. His mass-energy equivalence formula E = mc^2, which arises from relativity theory, has been called the world's most famous equation. He received the Nobel Prize in Physics in 1921 for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect.\",\n     \"question\": \"What year did Einstein receive the Nobel Prize?\", \"answer\": \"1921\", \"answer_variations\": [\"1921\", \"in 1921\"]},\n    {\"context\": \"The Great Wall of China is a series of fortifications that were built across the historical northern borders of ancient Chinese states and Imperial China as protection against various nomadic groups from the Eurasian Steppe. Several walls were built from as early as the 7th century BC, with selective stretches later joined together by Qin Shi Huang, the first emperor of China. The most well-known sections of the wall were built by the Ming dynasty from 1368 to 1644. The wall spans approximately 21,196 kilometers (13,171 miles).\",\n     \"question\": \"Who first joined the stretches of the wall together?\", \"answer\": \"Qin Shi Huang\", \"answer_variations\": [\"qin shi huang\", \"qin\", \"emperor qin\"]},\n    {\"context\": \"William Shakespeare was an English playwright, poet, and actor, widely regarded as the greatest writer in the English language and the world's greatest dramatist. He is often called England's national poet and the 'Bard of Avon'. His extant works, including collaborations, consist of some 39 plays, 154 sonnets, three long narrative poems, and a few other verses. His plays have been translated into every major living language and are performed more often than those of any other playwright.\",\n     \"question\": \"How many plays did Shakespeare write?\", \"answer\": \"39\", \"answer_variations\": [\"39\", \"39 plays\", \"some 39\", \"thirty-nine\"]},\n    {\"context\": \"The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as the best known, the most visited, the most written about, the most sung about, and the most parodied work of art in the world. The painting's novel qualities include the subject's enigmatic expression, monumentality of the composition, the subtle modelling of forms, and the atmospheric illusionism. The painting is held in the Louvre Museum in Paris since 1797.\",\n     \"question\": \"Who painted the Mona Lisa?\", \"answer\": \"Leonardo da Vinci\", \"answer_variations\": [\"leonardo da vinci\", \"da vinci\", \"leonardo\"]},\n    {\"context\": \"Mount Everest is Earth's highest mountain above sea level, located in the Mahalangur Himal sub-range of the Himalayas. The China-Nepal border runs across its summit point. Its elevation of 8,848.86 m (29,031 ft) was most recently established in 2020 by the Chinese and Nepali authorities. Mount Everest attracts many climbers, including highly experienced mountaineers. There are two main climbing routes, one approaching the summit from the southeast in Nepal and the other from the north in Tibet.\",\n     \"question\": \"What is the height of Mount Everest in meters?\", \"answer\": \"8848\", \"answer_variations\": [\"8848\", \"8,848\", \"8848.86\"]},\n    {\"context\": \"The Pacific Ocean is the largest and deepest of Earth's five oceanic divisions. It extends from the Arctic Ocean in the north to the Southern Ocean in the south, and is bounded by the continents of Asia and Australia in the west and the Americas in the east. At 165,250,000 square kilometers in area, this largest division of the World Ocean covers about 46% of Earth's water surface and about 32% of its total surface area.\",\n     \"question\": \"What percentage of Earth's water surface does the Pacific Ocean cover?\", \"answer\": \"46%\", \"answer_variations\": [\"46\", \"46%\", \"forty-six\"]},\n    {\"context\": \"The human brain is the central organ of the human nervous system, and with the spinal cord makes up the central nervous system. The brain consists of the cerebrum, the brainstem and the cerebellum. It controls most of the activities of the body, processing, integrating, and coordinating the information it receives from the sense organs. The average adult human brain weighs approximately 1.4 kilograms and contains about 86 billion neurons.\",\n     \"question\": \"How many neurons does the human brain contain?\", \"answer\": \"86 billion\", \"answer_variations\": [\"86 billion\", \"86billion\", \"eighty-six billion\", \"86\"]},\n    {\"context\": \"The Nile is a major north-flowing river in northeastern Africa. It flows into the Mediterranean Sea. The Nile is the longest river in Africa and has historically been considered the longest river in the world, though this has been contested by research suggesting that the Amazon River is slightly longer. The Nile's length is approximately 6,650 km (4,130 miles). It passes through eleven countries: Tanzania, Uganda, Rwanda, Burundi, the Democratic Republic of the Congo, Kenya, Ethiopia, Eritrea, South Sudan, Sudan, and Egypt.\",\n     \"question\": \"How many countries does the Nile pass through?\", \"answer\": \"eleven\", \"answer_variations\": [\"eleven\", \"11\", \"11 countries\"]},\n    {\"context\": \"Marie Curie was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields. She was born Maria Sklodowska in Warsaw in 1867 and later moved to Paris to study.\",\n     \"question\": \"What city was Marie Curie born in?\", \"answer\": \"Warsaw\", \"answer_variations\": [\"warsaw\"]},\n    {\"context\": \"The Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1632 by the Mughal emperor Shah Jahan to house the tomb of his favourite wife, Mumtaz Mahal. It also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare complex, which includes a mosque and a guest house.\",\n     \"question\": \"Who commissioned the construction of the Taj Mahal?\", \"answer\": \"Shah Jahan\", \"answer_variations\": [\"shah jahan\", \"shahjahan\"]},\n    {\"context\": \"The speed of light in vacuum, commonly denoted c, is a universal physical constant that is important in many areas of physics. The speed of light c is exactly equal to 299,792,458 metres per second (approximately 300,000 kilometres per second; 186,000 miles per second). According to the special theory of relativity, c is the upper limit for the speed at which conventional matter or energy can travel through space.\",\n     \"question\": \"What is the speed of light in kilometers per second?\", \"answer\": \"300,000\", \"answer_variations\": [\"300000\", \"300,000\", \"299792\", \"300\"]},\n    {\"context\": \"The Industrial Revolution, which took place from the 18th to 19th centuries, was a period during which predominantly agrarian, rural societies in Europe and America became industrial and urban. Prior to the Industrial Revolution, manufacturing was often done in people's homes, using hand tools or basic machines. Industrialization marked a shift to powered, special-purpose machinery, factories and mass production. The textile industry was the first to use modern production methods.\",\n     \"question\": \"Which industry was the first to use modern production methods?\", \"answer\": \"textile\", \"answer_variations\": [\"textile\", \"textiles\", \"the textile industry\"]},\n    {\"context\": \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that, through cellular respiration, can later be released to fuel the organism's activities. This chemical energy is stored in carbohydrate molecules, such as sugars and starches, which are synthesized from carbon dioxide and water. In most cases, oxygen is released as a waste product. The process occurs primarily in the chloroplasts of plant cells.\",\n     \"question\": \"Where in plant cells does photosynthesis primarily occur?\", \"answer\": \"chloroplasts\", \"answer_variations\": [\"chloroplasts\", \"chloroplast\", \"the chloroplasts\"]},\n    {\"context\": \"The Wright brothers, Orville and Wilbur, were two American aviation pioneers generally credited with inventing, building, and flying the world's first successful motor-operated airplane. They made the first controlled, sustained flight of a powered, heavier-than-air aircraft with the Wright Flyer on December 17, 1903, four miles south of Kitty Hawk, North Carolina.\",\n     \"question\": \"In what year did the Wright brothers make their first flight?\", \"answer\": \"1903\", \"answer_variations\": [\"1903\", \"in 1903\"]},\n    {\"context\": \"The Amazon River is the largest river by discharge volume of water in the world, and the disputed longest. The headwaters of the Apurimac River on Nevado Mismi had been considered for nearly a century as the Amazon's most distant source. The Amazon basin is the largest drainage basin in the world, with an area of approximately 7,000,000 square kilometres.\",\n     \"question\": \"What is the area of the Amazon basin in square kilometres?\", \"answer\": \"7,000,000\", \"answer_variations\": [\"7000000\", \"7,000,000\", \"7 million\"]},\n    {\"context\": \"Vincent van Gogh was a Dutch Post-Impressionist painter who posthumously became one of the most famous and influential figures in Western art history. In just over a decade, he created about 2,100 artworks, including around 860 oil paintings. He was born in Groot-Zundert in the Netherlands in 1853 and died in France in 1890 at the age of 37.\",\n     \"question\": \"How many oil paintings did Van Gogh create?\", \"answer\": \"860\", \"answer_variations\": [\"860\", \"around 860\", \"about 860\"]},\n    {\"context\": \"DNA, or deoxyribonucleic acid, is a molecule composed of two polynucleotide chains that coil around each other to form a double helix. DNA carries genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses. The structure of DNA was first described by James Watson and Francis Crick in 1953, based on X-ray diffraction data collected by Rosalind Franklin.\",\n     \"question\": \"Who first described the structure of DNA?\", \"answer\": \"Watson and Crick\", \"answer_variations\": [\"watson and crick\", \"james watson\", \"francis crick\", \"watson\", \"crick\"]},\n    {\"context\": \"The Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy. Built of travertine limestone, tuff, and brick-faced concrete, it was the largest amphitheatre ever built at the time and held 50,000 to 80,000 spectators. Construction began under Emperor Vespasian in AD 72 and was completed in AD 80 under his successor Titus.\",\n     \"question\": \"Under which emperor did construction of the Colosseum begin?\", \"answer\": \"Vespasian\", \"answer_variations\": [\"vespasian\", \"emperor vespasian\"]},\n]\n\n\n@torch.no_grad()\ndef evaluate_qa_accuracy_with_compression(\n    model, tokenizer, qa_examples, scorer, scorer_name,\n    retention_ratio=0.5, min_cache=48, device='cuda', use_compression=True,\n):\n    model.eval()\n\n    num_attention_heads = model.config.num_attention_heads\n    num_kv_heads = getattr(model.config, 'num_key_value_heads', num_attention_heads)\n    head_group_size = num_attention_heads // num_kv_heads\n\n    def group_attention_heads(attn):\n        if head_group_size == 1:\n            return attn\n        batch, heads, *rest = attn.shape\n        return attn.view(batch, num_kv_heads, head_group_size, *rest).mean(dim=2)\n\n    correct = 0\n    total = 0\n    results_detail = []\n\n    for qa in qa_examples:\n        if hasattr(scorer, 'reset'):\n            scorer.reset()\n\n        prompt = f\"\"\"Context: {qa['context']}\n\nQuestion: {qa['question']}\nAnswer (one word or phrase):\"\"\"\n\n        tokens = tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n        context_len = tokens.shape[1]\n\n        outputs = model(tokens, use_cache=True, output_attentions=True, return_dict=True)\n        past_kv = outputs.past_key_values\n\n        first_token_id = outputs.logits[:, -1, :].argmax(dim=-1).item()\n\n        if use_compression:\n            keys, values = get_cache_keys_values(past_kv, 0)\n            prefill_attention = outputs.attentions[-1]\n            prefill_attention_grouped = group_attention_heads(prefill_attention)\n\n            if isinstance(scorer, PrefillAttentionScorer):\n                importance = scorer(keys, values, prefill_attention_grouped)\n            else:\n                importance = scorer(keys, values, None)\n\n            if importance.dim() == 3:\n                avg_importance = importance.mean(dim=(0, 1))\n            elif importance.dim() == 2:\n                avg_importance = importance.mean(dim=0)\n            else:\n                avg_importance = importance.flatten()\n\n            budget = max(int(context_len * retention_ratio), min_cache)\n            budget = min(budget, context_len)\n\n            keep_mask = torch.zeros(context_len, dtype=torch.bool, device=device)\n            sink_size = 4\n            recent_size = min(16, context_len - sink_size)\n            keep_mask[:sink_size] = True\n            keep_mask[-recent_size:] = True\n\n            reserved = keep_mask.sum().item()\n            remaining_budget = max(0, budget - reserved)\n\n            if remaining_budget > 0:\n                importance_masked = avg_importance.clone()\n                importance_masked[keep_mask] = -float('inf')\n                k = min(remaining_budget, (~keep_mask).sum().item())\n                if k > 0:\n                    _, top_indices = importance_masked.topk(k)\n                    keep_mask[top_indices] = True\n\n            keep_indices = keep_mask.nonzero(as_tuple=True)[0].sort()[0]\n\n            compressed_kv = DynamicCache()\n            for layer_idx in range(get_cache_length(past_kv)):\n                layer_keys, layer_values = get_cache_keys_values(past_kv, layer_idx)\n                compressed_kv.update(\n                    layer_keys.index_select(2, keep_indices),\n                    layer_values.index_select(2, keep_indices),\n                    layer_idx\n                )\n            current_kv = compressed_kv\n        else:\n            current_kv = past_kv\n\n        generated_tokens = [first_token_id]\n        current_pos = context_len\n\n        for _ in range(19):\n            if generated_tokens[-1] == tokenizer.eos_token_id:\n                break\n\n            position_ids = torch.tensor([[current_pos]], device=device)\n            input_token = torch.tensor([[generated_tokens[-1]]], device=device)\n\n            out = model(\n                input_token,\n                past_key_values=current_kv,\n                position_ids=position_ids,\n                use_cache=True,\n                return_dict=True,\n            )\n\n            next_token = out.logits[:, -1, :].argmax(dim=-1).item()\n            generated_tokens.append(next_token)\n            current_kv = out.past_key_values\n            current_pos += 1\n\n            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n            if '\\n' in decoded or (len(decoded) > 5 and decoded.endswith('.')):\n                break\n\n        generated_answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n        generated_lower = generated_answer.lower()\n\n        is_correct = any(var in generated_lower for var in qa['answer_variations'])\n\n        if is_correct:\n            correct += 1\n        total += 1\n\n        results_detail.append({\n            'question': qa['question'],\n            'expected': qa['answer'],\n            'generated': generated_answer,\n            'correct': is_correct,\n        })\n\n    accuracy = correct / total if total > 0 else 0\n    return {'accuracy': accuracy, 'correct': correct, 'total': total, 'details': results_detail}\n\n\nqa_scorers = {\n    'SIP (Ours)': SIPPreGenerationScorer(sip_scorer),\n    'Prefill-Attn': PrefillAttentionScorer(),\n    'Position-Heuristic': PositionHeuristicScorer(),\n    'Expected-Attn': ExpectedAttentionPreGenScorer(),\n    'TRIM-KV': TRIMKVPreGenScorer(),\n    'Random': RandomImportanceScorer(),\n}\n\nqa_retention_ratios = [0.10, 0.25, 0.50, 0.75]\nqa_results = {}\n\nprint(\"Baseline (Full Cache)\")\nbaseline_result = evaluate_qa_accuracy_with_compression(\n    model=model, tokenizer=tokenizer, qa_examples=qa_examples,\n    scorer=RandomImportanceScorer(), scorer_name=\"Baseline\",\n    retention_ratio=1.0, min_cache=48, device=str(device), use_compression=False,\n)\nqa_results['Full Cache'] = {1.0: baseline_result['accuracy']}\nprint(f\"Full Cache: {baseline_result['accuracy']*100:.1f}% ({baseline_result['correct']}/{baseline_result['total']})\")\n\nfor scorer_name, scorer in qa_scorers.items():\n    qa_results[scorer_name] = {}\n\n    for ratio in qa_retention_ratios:\n        result = evaluate_qa_accuracy_with_compression(\n            model=model, tokenizer=tokenizer, qa_examples=qa_examples,\n            scorer=scorer, scorer_name=scorer_name,\n            retention_ratio=ratio, min_cache=48, device=str(device), use_compression=True,\n        )\n\n        qa_results[scorer_name][ratio] = result['accuracy']\n        print(f\"{scorer_name:<20} @ {int(ratio*100):>2}%: {result['accuracy']*100:.1f}% ({result['correct']}/{result['total']})\")\n\nprint(\"\\nQA Accuracy Summary\")\n\nheader = f\"{'Method':<20} |\"\nfor ratio in qa_retention_ratios:\n    header += f\" {int(ratio*100):>6}% |\"\nheader += \" Full |\"\nprint(header)\nprint(\"-\"*75)\n\nmethod_order = ['SIP (Ours)', 'Prefill-Attn', 'Position-Heuristic', 'Expected-Attn', 'TRIM-KV', 'Random']\nfor method in method_order:\n    if method in qa_results:\n        row = f\"{method:<20} |\"\n        for ratio in qa_retention_ratios:\n            if ratio in qa_results[method]:\n                acc = qa_results[method][ratio] * 100\n                row += f\" {acc:>5.0f}% |\"\n            else:\n                row += f\" {'--':>6} |\"\n        row += f\" {baseline_result['accuracy']*100:>4.0f}% |\"\n        print(row)\n\nprint(\"\\nBest Method at Each Retention Level\")\nfor ratio in qa_retention_ratios:\n    best_method = None\n    best_acc = -1\n    for method in method_order:\n        if method in qa_results and ratio in qa_results[method]:\n            acc = qa_results[method][ratio]\n            if acc > best_acc:\n                best_acc = acc\n                best_method = method\n    if best_method:\n        print(f\"  {int(ratio*100):>2}%: {best_method} ({best_acc*100:.0f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bz8b7uk5l8",
   "metadata": {},
   "outputs": [],
   "source": "def generate_haystack(length_tokens=500, tokenizer=None):\n    filler_sentences = [\n        \"The weather has been quite pleasant this season with moderate temperatures.\",\n        \"Many scientists are working on new discoveries in various fields of study.\",\n        \"Technology continues to advance at a rapid pace in modern society.\",\n        \"Education remains an important aspect of human development worldwide.\",\n        \"Various animals inhabit different ecosystems across the planet.\",\n        \"Music and art have been part of human culture for thousands of years.\",\n        \"Agriculture provides the foundation for food production globally.\",\n        \"Transportation systems connect cities and countries together.\",\n        \"Healthcare research aims to improve quality of life for everyone.\",\n        \"Architecture reflects the cultural values of different societies.\",\n    ]\n\n    haystack = \"\"\n    while len(tokenizer.encode(haystack)) < length_tokens:\n        haystack += np.random.choice(filler_sentences) + \" \"\n\n    return haystack.strip()\n\n\ndef create_needle_test(needle_fact, needle_question, haystack_length=500, needle_position=0.5, tokenizer=None):\n    part1_len = int(haystack_length * needle_position)\n    part2_len = haystack_length - part1_len\n\n    part1 = generate_haystack(part1_len, tokenizer) if part1_len > 0 else \"\"\n    part2 = generate_haystack(part2_len, tokenizer) if part2_len > 0 else \"\"\n\n    if part1:\n        context = part1 + \" \" + needle_fact + \" \" + part2\n    else:\n        context = needle_fact + \" \" + part2\n\n    return {\n        'context': context.strip(),\n        'question': needle_question,\n        'answer': needle_fact,\n    }\n\n\nneedle_tests = [\n    {\n        'needle': \"The secret code for the vault is XRAY-42-BETA.\",\n        'question': \"What is the secret code for the vault?\",\n        'answer_key': \"XRAY-42-BETA\",\n    },\n    {\n        'needle': \"Dr. Alexandra Chen discovered the new element on March 15th, 2024.\",\n        'question': \"Who discovered the new element?\",\n        'answer_key': \"Alexandra Chen\",\n    },\n    {\n        'needle': \"The ancient artifact was hidden in the Temple of the Golden Sun.\",\n        'question': \"Where was the ancient artifact hidden?\",\n        'answer_key': \"Temple of the Golden Sun\",\n    },\n    {\n        'needle': \"The winning lottery numbers for the grand prize were 7, 23, 45, 62, 89.\",\n        'question': \"What were the winning lottery numbers?\",\n        'answer_key': \"7, 23, 45, 62, 89\",\n    },\n    {\n        'needle': \"Captain Marcus Webb commanded the first Mars expedition in 2031.\",\n        'question': \"Who commanded the first Mars expedition?\",\n        'answer_key': \"Marcus Webb\",\n    },\n    {\n        'needle': \"The password to access the mainframe is OMEGA-SEVEN-DELTA.\",\n        'question': \"What is the password to access the mainframe?\",\n        'answer_key': \"OMEGA-SEVEN-DELTA\",\n    },\n    {\n        'needle': \"Professor Elena Vasquez invented the quantum processor in Helsinki.\",\n        'question': \"Who invented the quantum processor?\",\n        'answer_key': \"Elena Vasquez\",\n    },\n    {\n        'needle': \"The treasure map coordinates are 47.3N, 122.5W near Seattle.\",\n        'question': \"What are the treasure map coordinates?\",\n        'answer_key': \"47.3N, 122.5W\",\n    },\n]\n\n\n@torch.no_grad()\ndef evaluate_needle_in_haystack(\n    model, tokenizer, needle_tests, scorer, scorer_name,\n    haystack_length=400, retention_ratio=0.5, \n    needle_positions=[0.2, 0.5, 0.8], device='cuda',\n    use_compression=True,\n):\n    model.eval()\n\n    num_attention_heads = model.config.num_attention_heads\n    num_kv_heads = getattr(model.config, 'num_key_value_heads', num_attention_heads)\n    head_group_size = num_attention_heads // num_kv_heads\n\n    def group_attention_heads(attn):\n        if head_group_size == 1:\n            return attn\n        batch, heads, *rest = attn.shape\n        return attn.view(batch, num_kv_heads, head_group_size, *rest).mean(dim=2)\n\n    results_by_position = {pos: {'correct': 0, 'total': 0} for pos in needle_positions}\n\n    for needle_test in needle_tests:\n        for position in needle_positions:\n            if hasattr(scorer, 'reset'):\n                scorer.reset()\n\n            test_case = create_needle_test(\n                needle_fact=needle_test['needle'],\n                needle_question=needle_test['question'],\n                haystack_length=haystack_length,\n                needle_position=position,\n                tokenizer=tokenizer,\n            )\n\n            prompt = f\"\"\"Context: {test_case['context']}\n\nQuestion: {test_case['question']}\nAnswer:\"\"\"\n\n            tokens = tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n            context_len = tokens.shape[1]\n\n            if context_len < 100:\n                continue\n\n            outputs = model(tokens, use_cache=True, output_attentions=True, return_dict=True)\n            past_kv = outputs.past_key_values\n\n            first_token_id = outputs.logits[:, -1, :].argmax(dim=-1).item()\n\n            if use_compression:\n                keys, values = get_cache_keys_values(past_kv, 0)\n                prefill_attention = outputs.attentions[-1]\n                prefill_attention_grouped = group_attention_heads(prefill_attention)\n\n                if isinstance(scorer, PrefillAttentionScorer):\n                    importance = scorer(keys, values, prefill_attention_grouped)\n                else:\n                    importance = scorer(keys, values, None)\n\n                if importance.dim() == 3:\n                    avg_importance = importance.mean(dim=(0, 1))\n                elif importance.dim() == 2:\n                    avg_importance = importance.mean(dim=0)\n                else:\n                    avg_importance = importance.flatten()\n\n                budget = max(int(context_len * retention_ratio), 48)\n                keep_mask = torch.zeros(context_len, dtype=torch.bool, device=device)\n                keep_mask[:4] = True\n                keep_mask[-16:] = True\n\n                remaining = max(0, budget - keep_mask.sum().item())\n                if remaining > 0:\n                    importance_masked = avg_importance.clone()\n                    importance_masked[keep_mask] = -float('inf')\n                    k = min(remaining, (~keep_mask).sum().item())\n                    if k > 0:\n                        _, top_idx = importance_masked.topk(k)\n                        keep_mask[top_idx] = True\n\n                keep_indices = keep_mask.nonzero(as_tuple=True)[0].sort()[0]\n\n                compressed_kv = DynamicCache()\n                for layer_idx in range(get_cache_length(past_kv)):\n                    lk, lv = get_cache_keys_values(past_kv, layer_idx)\n                    compressed_kv.update(lk.index_select(2, keep_indices), lv.index_select(2, keep_indices), layer_idx)\n\n                current_kv = compressed_kv\n            else:\n                current_kv = past_kv\n\n            generated_tokens = [first_token_id]\n            current_pos = context_len\n\n            for _ in range(29):\n                if generated_tokens[-1] == tokenizer.eos_token_id:\n                    break\n\n                position_ids = torch.tensor([[current_pos]], device=device)\n                input_token = torch.tensor([[generated_tokens[-1]]], device=device)\n\n                out = model(input_token, past_key_values=current_kv, position_ids=position_ids, use_cache=True, return_dict=True)\n                next_token = out.logits[:, -1, :].argmax(dim=-1).item()\n\n                generated_tokens.append(next_token)\n                current_kv = out.past_key_values\n                current_pos += 1\n\n                decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n                if '\\n' in decoded or len(decoded) > 50:\n                    break\n\n            generated = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip().lower()\n            answer_key = needle_test['answer_key'].lower()\n\n            is_correct = answer_key in generated or any(word in generated for word in answer_key.split()[:2])\n\n            results_by_position[position]['total'] += 1\n            if is_correct:\n                results_by_position[position]['correct'] += 1\n\n    return {\n        pos: data['correct'] / max(1, data['total'])\n        for pos, data in results_by_position.items()\n    }\n\n\nneedle_scorers = {\n    'SIP (Ours)': SIPPreGenerationScorer(sip_scorer),\n    'Prefill-Attn': PrefillAttentionScorer(),\n    'Position-Heuristic': PositionHeuristicScorer(),\n    'Expected-Attn': ExpectedAttentionPreGenScorer(),\n    'TRIM-KV': TRIMKVPreGenScorer(),\n    'Random': RandomImportanceScorer(),\n}\n\nneedle_retention_ratios = [0.25, 0.50, 0.75]\nneedle_positions = [0.2, 0.5, 0.8]\nneedle_results = {}\n\nprint(\"Baseline (Full Cache)\")\nbaseline_needle = evaluate_needle_in_haystack(\n    model=model, tokenizer=tokenizer, needle_tests=needle_tests,\n    scorer=RandomImportanceScorer(), scorer_name=\"Baseline\",\n    haystack_length=350, retention_ratio=1.0,\n    needle_positions=needle_positions, device=str(device),\n    use_compression=False,\n)\nneedle_results['Full Cache'] = {1.0: baseline_needle}\nprint(f\"Full Cache: Start={baseline_needle[0.2]*100:.0f}% Middle={baseline_needle[0.5]*100:.0f}% End={baseline_needle[0.8]*100:.0f}%\")\n\nfor scorer_name, scorer in needle_scorers.items():\n    needle_results[scorer_name] = {}\n\n    for ratio in needle_retention_ratios:\n        result = evaluate_needle_in_haystack(\n            model=model, tokenizer=tokenizer, needle_tests=needle_tests,\n            scorer=scorer, scorer_name=scorer_name,\n            haystack_length=350, retention_ratio=ratio,\n            needle_positions=needle_positions, device=str(device),\n            use_compression=True,\n        )\n\n        needle_results[scorer_name][ratio] = result\n\nprint(\"\\nNeedle-in-Haystack Results\")\nprint(f\"Baseline (Full Cache): Start={baseline_needle[0.2]*100:.0f}% Middle={baseline_needle[0.5]*100:.0f}% End={baseline_needle[0.8]*100:.0f}%\")\n\nmethod_order = ['SIP (Ours)', 'Prefill-Attn', 'Position-Heuristic', 'Expected-Attn', 'TRIM-KV', 'Random']\n\nfor ratio in needle_retention_ratios:\n    print(f\"\\n@ {int(ratio*100)}% Retention\")\n    print(f\"{'Method':<20} | {'Start':>8} | {'Middle':>8} | {'End':>8} | {'Avg':>8}\")\n    print(\"-\"*60)\n\n    for method in method_order:\n        if method in needle_results and ratio in needle_results[method]:\n            res = needle_results[method][ratio]\n            avg = np.mean([res[0.2], res[0.5], res[0.8]])\n            print(f\"{method:<20} | {res[0.2]*100:>6.0f}% | {res[0.5]*100:>6.0f}% | {res[0.8]*100:>6.0f}% | {avg*100:>6.0f}%\")\n\nprint(\"\\nBest Method by Retention Level (Average across positions)\")\n\nfor ratio in needle_retention_ratios:\n    best_method = None\n    best_avg = -1\n    for method in method_order:\n        if method in needle_results and ratio in needle_results[method]:\n            res = needle_results[method][ratio]\n            avg = np.mean([res[0.2], res[0.5], res[0.8]])\n            if avg > best_avg:\n                best_avg = avg\n                best_method = method\n    if best_method:\n        print(f\"  {int(ratio*100):>2}%: {best_method} ({best_avg*100:.0f}% avg)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zn4u9098cn",
   "metadata": {},
   "outputs": [],
   "source": "class KeysOnlyScorer:\n    def __init__(self):\n        pass\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, head_dim = keys.shape\n        key_norms = keys.float().norm(dim=-1)\n        importance = key_norms / (key_norms.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance\n\n\nclass ValuesOnlyScorer:\n    def __init__(self):\n        pass\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, head_dim = values.shape\n        value_norms = values.float().norm(dim=-1)\n        importance = value_norms / (value_norms.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance\n\n\nclass KeysValuesScorer:\n    def __init__(self):\n        pass\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, head_dim = keys.shape\n        key_norms = keys.float().norm(dim=-1)\n        value_norms = values.float().norm(dim=-1)\n\n        recent_keys = keys[:, :, -32:, :].float()\n        recent_mean = recent_keys.mean(dim=2)\n\n        similarity = torch.einsum('bhsd,bhd->bhs', keys.float(), recent_mean)\n        similarity = similarity / (head_dim ** 0.5)\n\n        importance = (key_norms + value_norms) / 2 + 0.5 * torch.sigmoid(similarity)\n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance\n\n\nclass KVPositionScorer:\n    def __init__(self):\n        pass\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n\n        key_norms = keys.float().norm(dim=-1)\n        value_norms = values.float().norm(dim=-1)\n        kv_score = (key_norms + value_norms) / 2\n\n        positions = torch.arange(seq_len, device=device, dtype=torch.float32)\n        recency = positions / seq_len\n\n        sink_bonus = torch.zeros(seq_len, device=device)\n        sink_bonus[:4] = 1.0\n\n        position_score = 0.3 * recency + 0.2 * sink_bonus\n        position_score = position_score.unsqueeze(0).unsqueeze(0)\n\n        importance = kv_score + position_score\n        importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n        return importance\n\n\nclass AttentionOnlyScorer:\n    def __init__(self):\n        self.prefill_attention = None\n\n    def reset(self):\n        self.prefill_attention = None\n\n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, _ = keys.shape\n        device = keys.device\n\n        if attention is not None:\n            if attention.dim() == 4:\n                self.prefill_attention = attention.mean(dim=2)\n            else:\n                self.prefill_attention = attention\n\n        if self.prefill_attention is not None:\n            importance = self.prefill_attention\n            importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n            return importance\n        else:\n            return torch.ones(batch, heads, seq_len, device=device)\n\n\nclass SingleStepSIPScorer:\n    def __init__(self, sip_model):\n        self.model = sip_model\n        self.model.eval()\n\n    def reset(self):\n        pass\n\n    def __call__(self, keys, values, attention=None):\n        with torch.no_grad():\n            positions = torch.arange(keys.shape[2], device=keys.device).unsqueeze(0)\n            kv_features = self.model.kv_encoder(keys.float(), values.float())\n            pred, conf = self.model.predictor(kv_features, positions, keys.shape[2], lookahead_steps=1)\n            return pred[:, :, :, 0]\n\n\nablation_scorers = {\n    'Random': RandomImportanceScorer(),\n    'Attention-Only': AttentionOnlyScorer(),\n    'Prefill-Attn': PrefillAttentionScorer(),\n    'Position-Heuristic': PositionHeuristicScorer(),\n    'Keys-Only': KeysOnlyScorer(),\n    'Values-Only': ValuesOnlyScorer(),\n    'Keys+Values': KeysValuesScorer(),\n    'K+V+Position': KVPositionScorer(),\n    'SIP-SingleStep': SingleStepSIPScorer(sip_scorer),\n    'SIP-Full': SIPPreGenerationScorer(sip_scorer),\n}\n\nablation_ratios = [0.25, 0.50, 0.75]\nablation_results = {name: {} for name in ablation_scorers.keys()}\n\nnum_attention_heads = model.config.num_attention_heads\nnum_kv_heads = getattr(model.config, 'num_key_value_heads', num_attention_heads)\nhead_group_size = num_attention_heads // num_kv_heads\n\ndef group_attention_heads(attn):\n    if head_group_size == 1:\n        return attn\n    batch, heads, *rest = attn.shape\n    return attn.view(batch, num_kv_heads, head_group_size, *rest).mean(dim=2)\n\nprint(\"Computing Full Cache Baseline\")\nbaseline_nll = 0.0\nbaseline_tokens = 0\n\nfor text in tqdm(test_texts[:30], desc=\"Baseline\"):\n    tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n    seq_len = tokens.shape[1]\n    if seq_len < 128:\n        continue\n\n    split_point = seq_len * 3 // 4\n    context_ids = tokens[:, :split_point]\n    continuation_ids = tokens[:, split_point:]\n\n    with torch.no_grad():\n        outputs = model(context_ids, use_cache=True, return_dict=True)\n        current_kv = outputs.past_key_values\n        current_pos = split_point\n\n        for i in range(continuation_ids.shape[1] - 1):\n            input_token = continuation_ids[:, i:i+1]\n            target_token = continuation_ids[:, i+1]\n            position_ids = torch.tensor([[current_pos]], device=device)\n\n            out = model(input_token, past_key_values=current_kv, position_ids=position_ids,\n                        use_cache=True, return_dict=True)\n            loss = F.cross_entropy(out.logits[:, -1, :], target_token, reduction='sum')\n\n            if not torch.isnan(loss):\n                baseline_nll += loss.item()\n                baseline_tokens += 1\n\n            current_kv = out.past_key_values\n            current_pos += 1\n\nbaseline_ppl = np.exp(baseline_nll / baseline_tokens) if baseline_tokens > 0 else float('inf')\nprint(f\"Full Cache Baseline PPL: {baseline_ppl:.2f}\")\n\nfor scorer_name, scorer in ablation_scorers.items():\n    for ratio in ablation_ratios:\n        total_nll = 0.0\n        total_tokens = 0\n\n        for text in test_texts[:30]:\n            if hasattr(scorer, 'reset'):\n                scorer.reset()\n\n            tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n            seq_len = tokens.shape[1]\n            if seq_len < 128:\n                continue\n\n            split_point = seq_len * 3 // 4\n            context_ids = tokens[:, :split_point]\n            continuation_ids = tokens[:, split_point:]\n\n            with torch.no_grad():\n                outputs = model(context_ids, use_cache=True, output_attentions=True, return_dict=True)\n                past_kv = outputs.past_key_values\n\n                keys, values = get_cache_keys_values(past_kv, 0)\n                prefill_attention = outputs.attentions[-1]\n                prefill_attention_grouped = group_attention_heads(prefill_attention)\n\n                if isinstance(scorer, (AttentionOnlyScorer, PrefillAttentionScorer)):\n                    importance = scorer(keys, values, prefill_attention_grouped)\n                else:\n                    importance = scorer(keys, values, None)\n\n                if importance.dim() == 3:\n                    avg_importance = importance.mean(dim=(0, 1))\n                elif importance.dim() == 2:\n                    avg_importance = importance.mean(dim=0)\n                else:\n                    avg_importance = importance.flatten()\n\n                budget = max(int(split_point * ratio), 48)\n                keep_mask = torch.zeros(split_point, dtype=torch.bool, device=device)\n                keep_mask[:4] = True\n                keep_mask[-16:] = True\n\n                remaining = max(0, budget - keep_mask.sum().item())\n                if remaining > 0:\n                    importance_masked = avg_importance.clone()\n                    importance_masked[keep_mask] = -float('inf')\n                    k = min(remaining, (~keep_mask).sum().item())\n                    if k > 0:\n                        _, top_idx = importance_masked.topk(k)\n                        keep_mask[top_idx] = True\n\n                keep_indices = keep_mask.nonzero(as_tuple=True)[0].sort()[0]\n\n                compressed_kv = DynamicCache()\n                for layer_idx in range(get_cache_length(past_kv)):\n                    lk, lv = get_cache_keys_values(past_kv, layer_idx)\n                    compressed_kv.update(lk.index_select(2, keep_indices), lv.index_select(2, keep_indices), layer_idx)\n\n                current_kv = compressed_kv\n                current_pos = split_point\n\n                for i in range(continuation_ids.shape[1] - 1):\n                    input_token = continuation_ids[:, i:i+1]\n                    target_token = continuation_ids[:, i+1]\n                    position_ids = torch.tensor([[current_pos]], device=device)\n\n                    out = model(input_token, past_key_values=current_kv, position_ids=position_ids,\n                                use_cache=True, return_dict=True)\n                    logits = out.logits[:, -1, :]\n                    loss = F.cross_entropy(logits, target_token, reduction='sum')\n\n                    if not torch.isnan(loss):\n                        total_nll += loss.item()\n                        total_tokens += 1\n\n                    current_kv = out.past_key_values\n                    current_pos += 1\n\n        if total_tokens > 0:\n            ppl = np.exp(total_nll / total_tokens)\n            ablation_results[scorer_name][ratio] = ppl\n\nprint(\"\\nAblation Study Results\")\nprint(f\"Full Cache Baseline: {baseline_ppl:.2f}\")\n\nprint(f\"\\n{'Component':<20} |\", end=\"\")\nfor ratio in ablation_ratios:\n    print(f\" {int(ratio*100):>6}% |\", end=\"\")\nprint(\"\")\nprint(\"-\"*55)\n\nmethod_order = ['Random', 'Attention-Only', 'Prefill-Attn', 'Position-Heuristic',\n                'Keys-Only', 'Values-Only', 'Keys+Values', 'K+V+Position',\n                'SIP-SingleStep', 'SIP-Full']\n\nfor scorer_name in method_order:\n    if scorer_name in ablation_results:\n        row = f\"{scorer_name:<20} |\"\n        for ratio in ablation_ratios:\n            if ratio in ablation_results[scorer_name]:\n                ppl = ablation_results[scorer_name][ratio]\n                row += f\" {ppl:>6.2f} |\"\n            else:\n                row += f\" {'--':>6} |\"\n        print(row)\n\nprint(\"\\nComparison @ 50% Retention (vs Random and vs Attention-Only)\")\n\nrandom_ppl_50 = ablation_results.get('Random', {}).get(0.50, float('inf'))\nattn_only_ppl_50 = ablation_results.get('Attention-Only', {}).get(0.50, float('inf'))\n\nprint(f\"\\n{'Component':<20} | {'PPL':>8} | {'vs Random':>12} | {'vs Attn-Only':>14}\")\nprint(\"-\"*65)\n\nfor scorer_name in method_order:\n    if scorer_name in ablation_results and 0.50 in ablation_results[scorer_name]:\n        ppl = ablation_results[scorer_name][0.50]\n        vs_random = ((random_ppl_50 - ppl) / random_ppl_50) * 100 if random_ppl_50 != float('inf') else 0\n        vs_attn = ((attn_only_ppl_50 - ppl) / attn_only_ppl_50) * 100 if attn_only_ppl_50 != float('inf') else 0\n        print(f\"{scorer_name:<20} | {ppl:>8.2f} | {vs_random:>+10.1f}% | {vs_attn:>+12.1f}%\")\n\nsip_full_ppl = ablation_results.get('SIP-Full', {}).get(0.50, float('inf'))\nprefill_attn_ppl = ablation_results.get('Prefill-Attn', {}).get(0.50, float('inf'))\npos_heur_ppl = ablation_results.get('Position-Heuristic', {}).get(0.50, float('inf'))\n\nprint(\"\\nCritical Comparison: SIP vs Simple Baselines @ 50%\")\nprint(f\"  SIP-Full:           {sip_full_ppl:.2f}\")\nprint(f\"  Prefill-Attn:       {prefill_attn_ppl:.2f}\")\nprint(f\"  Position-Heuristic: {pos_heur_ppl:.2f}\")\nprint(f\"  Attention-Only:     {attn_only_ppl_50:.2f}\")\nprint(f\"  Random:             {random_ppl_50:.2f}\")\n\nprint(\"\\nMulti-Horizon Speculation Value @ 50%\")\nsingle_ppl = ablation_results.get('SIP-SingleStep', {}).get(0.50, float('inf'))\nfull_ppl = ablation_results.get('SIP-Full', {}).get(0.50, float('inf'))\n\nif single_ppl != float('inf') and full_ppl != float('inf'):\n    multi_horizon_gain = ((single_ppl - full_ppl) / single_ppl) * 100\n    print(f\"  Single-step SIP: {single_ppl:.2f}\")\n    print(f\"  Full SIP (8-step): {full_ppl:.2f}\")\n    print(f\"  Multi-horizon gain: {multi_horizon_gain:+.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jvlm6k1t8sa",
   "metadata": {},
   "outputs": [],
   "source": "RUN_TRANSFER_EXPERIMENT = False\n\nclass TransferableSIPScorer:\n    def __init__(self, sip_model, source_num_kv_heads=4, target_num_kv_heads=None):\n        self.model = sip_model\n        self.model.eval()\n        self.source_num_kv_heads = source_num_kv_heads\n        self.target_num_kv_heads = target_num_kv_heads or source_num_kv_heads\n    \n    def reset(self):\n        pass\n    \n    def _adapt_heads(self, tensor, source_heads, target_heads):\n        if source_heads == target_heads:\n            return tensor\n        \n        batch, heads, seq_len = tensor.shape\n        \n        if target_heads > source_heads:\n            repeat_factor = target_heads // source_heads\n            tensor = tensor.repeat_interleave(repeat_factor, dim=1)\n            remainder = target_heads % source_heads\n            if remainder > 0:\n                tensor = torch.cat([tensor, tensor[:, :remainder, :]], dim=1)\n        else:\n            group_size = source_heads // target_heads\n            tensor = tensor.view(batch, target_heads, group_size, seq_len).mean(dim=2)\n        \n        return tensor[:, :target_heads, :]\n    \n    def __call__(self, keys, values, attention=None):\n        batch, heads, seq_len, head_dim = keys.shape\n        device = keys.device\n        \n        with torch.no_grad():\n            expected_head_dim = self.model.config.head_dim\n            if head_dim != expected_head_dim:\n                importance = keys.float().norm(dim=-1) + values.float().norm(dim=-1)\n                importance = importance / (importance.max(dim=-1, keepdim=True)[0] + 1e-8)\n                return importance\n            \n            if heads != self.source_num_kv_heads:\n                group_size = heads // self.source_num_kv_heads\n                if group_size > 0:\n                    keys_adapted = keys.view(batch, self.source_num_kv_heads, group_size, seq_len, head_dim).mean(dim=2)\n                    values_adapted = values.view(batch, self.source_num_kv_heads, group_size, seq_len, head_dim).mean(dim=2)\n                else:\n                    keys_adapted = keys.repeat_interleave(self.source_num_kv_heads // heads + 1, dim=1)[:, :self.source_num_kv_heads, :, :]\n                    values_adapted = values.repeat_interleave(self.source_num_kv_heads // heads + 1, dim=1)[:, :self.source_num_kv_heads, :, :]\n            else:\n                keys_adapted = keys\n                values_adapted = values\n            \n            positions = torch.arange(seq_len, device=device).unsqueeze(0)\n            importance = self.model(keys_adapted.float(), values_adapted.float(), positions)\n            \n            importance = self._adapt_heads(importance, self.source_num_kv_heads, heads)\n        \n        return importance\n\n\n@torch.no_grad()\ndef evaluate_transfer(\n    source_sip_model,\n    target_model,\n    target_tokenizer,\n    texts,\n    source_config,\n    retention_ratio=0.5,\n    max_samples=20,\n    device='cuda',\n):\n    target_model.eval()\n    \n    target_num_heads = target_model.config.num_attention_heads\n    target_num_kv_heads = getattr(target_model.config, 'num_key_value_heads', target_num_heads)\n    target_head_dim = target_model.config.hidden_size // target_num_heads\n    \n    transfer_scorer = TransferableSIPScorer(\n        source_sip_model,\n        source_num_kv_heads=source_config.num_kv_heads,\n        target_num_kv_heads=target_num_kv_heads,\n    )\n    \n    random_scorer = RandomImportanceScorer()\n    \n    results = {'transfer': {'nll': 0, 'tokens': 0}, 'random': {'nll': 0, 'tokens': 0}}\n    \n    for text in tqdm(texts[:max_samples], desc=\"Transfer eval\"):\n        tokens = target_tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n        seq_len = tokens.shape[1]\n        if seq_len < 100:\n            continue\n        \n        split_point = seq_len * 3 // 4\n        context = tokens[:, :split_point]\n        continuation = tokens[:, split_point:]\n        \n        for scorer_name, scorer in [('transfer', transfer_scorer), ('random', random_scorer)]:\n            if hasattr(scorer, 'reset'):\n                scorer.reset()\n            \n            outputs = target_model(context, use_cache=True, return_dict=True)\n            past_kv = outputs.past_key_values\n            \n            keys, values = get_cache_keys_values(past_kv, 0)\n            importance = scorer(keys, values, None)\n            \n            if importance.dim() == 3:\n                avg_importance = importance.mean(dim=(0, 1))\n            else:\n                avg_importance = importance.flatten()\n            \n            budget = max(int(split_point * retention_ratio), 48)\n            keep_mask = torch.zeros(split_point, dtype=torch.bool, device=device)\n            keep_mask[:4] = True\n            keep_mask[-16:] = True\n            \n            remaining = max(0, budget - keep_mask.sum().item())\n            if remaining > 0:\n                importance_masked = avg_importance.clone()\n                importance_masked[keep_mask] = -float('inf')\n                k = min(remaining, (~keep_mask).sum().item())\n                if k > 0:\n                    _, top_idx = importance_masked.topk(k)\n                    keep_mask[top_idx] = True\n            \n            keep_indices = keep_mask.nonzero(as_tuple=True)[0].sort()[0]\n            \n            compressed_kv = DynamicCache()\n            for layer_idx in range(get_cache_length(past_kv)):\n                lk, lv = get_cache_keys_values(past_kv, layer_idx)\n                compressed_kv.update(lk.index_select(2, keep_indices), lv.index_select(2, keep_indices), layer_idx)\n            \n            current_kv = compressed_kv\n            current_pos = split_point\n            \n            for i in range(continuation.shape[1] - 1):\n                input_token = continuation[:, i:i+1]\n                target_token = continuation[:, i+1]\n                position_ids = torch.tensor([[current_pos]], device=device)\n                \n                out = target_model(input_token, past_key_values=current_kv, position_ids=position_ids,\n                                   use_cache=True, return_dict=True)\n                loss = F.cross_entropy(out.logits[:, -1, :], target_token, reduction='sum')\n                \n                if not torch.isnan(loss):\n                    results[scorer_name]['nll'] += loss.item()\n                    results[scorer_name]['tokens'] += 1\n                \n                current_kv = out.past_key_values\n                current_pos += 1\n    \n    for name in results:\n        if results[name]['tokens'] > 0:\n            results[name]['ppl'] = np.exp(results[name]['nll'] / results[name]['tokens'])\n        else:\n            results[name]['ppl'] = float('inf')\n    \n    return results\n\n\nif RUN_TRANSFER_EXPERIMENT:\n    print(\"Running Cross-Model Transfer Experiment\")\n    \n    try:\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        \n        target_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n        target_tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n        target_model = AutoModelForCausalLM.from_pretrained(\n            target_model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            attn_implementation=\"eager\",\n        )\n        target_model.eval()\n        target_tokenizer.pad_token = target_tokenizer.eos_token\n        \n        transfer_results = evaluate_transfer(\n            source_sip_model=sip_scorer,\n            target_model=target_model,\n            target_tokenizer=target_tokenizer,\n            texts=test_texts,\n            source_config=sip_config,\n            retention_ratio=0.5,\n            max_samples=20,\n            device=str(device),\n        )\n        \n        print(f\"\\nTarget Model: {target_model_name}\")\n        print(f\"SIP Transfer PPL: {transfer_results['transfer']['ppl']:.2f}\")\n        print(f\"Random PPL: {transfer_results['random']['ppl']:.2f}\")\n        \n        improvement = (transfer_results['random']['ppl'] - transfer_results['transfer']['ppl']) / transfer_results['random']['ppl'] * 100\n        print(f\"Transfer improvement over random: {improvement:+.1f}%\")\n        \n        del target_model\n        torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"Transfer experiment failed: {e}\")\nelse:\n    print(\"Transfer experiment skipped (RUN_TRANSFER_EXPERIMENT = False)\")\n\nprint(\"\\nSame-Architecture Transfer Test\")\n\ntransfer_test_texts = val_texts[-50:]\n\nsame_arch_results = {}\nfor scorer_name, scorer in [('SIP', SIPPreGenerationScorer(sip_scorer)), ('Random', RandomImportanceScorer())]:\n    total_nll = 0\n    total_tokens = 0\n    \n    for text in tqdm(transfer_test_texts[:20], desc=f\"Same-arch transfer ({scorer_name})\"):\n        if hasattr(scorer, 'reset'):\n            scorer.reset()\n        \n        tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n        seq_len = tokens.shape[1]\n        if seq_len < 100:\n            continue\n        \n        split_point = seq_len * 3 // 4\n        context = tokens[:, :split_point]\n        continuation = tokens[:, split_point:]\n        \n        with torch.no_grad():\n            outputs = model(context, use_cache=True, return_dict=True)\n            past_kv = outputs.past_key_values\n            \n            keys, values = get_cache_keys_values(past_kv, 0)\n            importance = scorer(keys, values, None)\n            \n            if importance.dim() == 3:\n                avg_importance = importance.mean(dim=(0, 1))\n            else:\n                avg_importance = importance.flatten()\n            \n            budget = max(int(split_point * 0.5), 48)\n            keep_mask = torch.zeros(split_point, dtype=torch.bool, device=device)\n            keep_mask[:4] = True\n            keep_mask[-16:] = True\n            \n            remaining = max(0, budget - keep_mask.sum().item())\n            if remaining > 0:\n                importance_masked = avg_importance.clone()\n                importance_masked[keep_mask] = -float('inf')\n                k = min(remaining, (~keep_mask).sum().item())\n                if k > 0:\n                    _, top_idx = importance_masked.topk(k)\n                    keep_mask[top_idx] = True\n            \n            keep_indices = keep_mask.nonzero(as_tuple=True)[0].sort()[0]\n            \n            compressed_kv = DynamicCache()\n            for layer_idx in range(get_cache_length(past_kv)):\n                lk, lv = get_cache_keys_values(past_kv, layer_idx)\n                compressed_kv.update(lk.index_select(2, keep_indices), lv.index_select(2, keep_indices), layer_idx)\n            \n            current_kv = compressed_kv\n            current_pos = split_point\n            \n            for i in range(continuation.shape[1] - 1):\n                input_token = continuation[:, i:i+1]\n                target_token = continuation[:, i+1]\n                position_ids = torch.tensor([[current_pos]], device=device)\n                \n                out = model(input_token, past_key_values=current_kv, position_ids=position_ids,\n                           use_cache=True, return_dict=True)\n                loss = F.cross_entropy(out.logits[:, -1, :], target_token, reduction='sum')\n                \n                if not torch.isnan(loss):\n                    total_nll += loss.item()\n                    total_tokens += 1\n                \n                current_kv = out.past_key_values\n                current_pos += 1\n    \n    if total_tokens > 0:\n        same_arch_results[scorer_name] = np.exp(total_nll / total_tokens)\n\nprint(\"\\nSame-Architecture Transfer Results\")\nprint(f\"{'Method':<15} | {'PPL':>10}\")\nprint(\"-\"*30)\nfor name, ppl in same_arch_results.items():\n    print(f\"{name:<15} | {ppl:>10.2f}\")\n\nif 'SIP' in same_arch_results and 'Random' in same_arch_results:\n    improvement = (same_arch_results['Random'] - same_arch_results['SIP']) / same_arch_results['Random'] * 100\n    print(f\"\\nSIP improvement on held-out data: {improvement:+.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 10. Lookahead Accuracy with GQA Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef evaluate_lookahead_accuracy(\n    model, tokenizer, sip_scorer, texts, \n    lookahead_steps=[1, 4, 8], num_samples=50, device='cuda'\n):\n    num_attention_heads = model.config.num_attention_heads\n    num_kv_heads = getattr(model.config, 'num_key_value_heads', num_attention_heads)\n    head_group_size = num_attention_heads // num_kv_heads\n\n    def group_heads(attn):\n        if head_group_size == 1:\n            return attn\n        if attn.dim() == 3:\n            b, h, s = attn.shape\n            return attn.view(b, num_kv_heads, head_group_size, s).mean(dim=2)\n        elif attn.dim() == 4:\n            b, h, q, k = attn.shape\n            return attn.view(b, num_kv_heads, head_group_size, q, k).mean(dim=2)\n        return attn\n\n    results = {s: {'pearson': [], 'spearman': [], 'topk_recall': []} for s in lookahead_steps}\n\n    for text in tqdm(texts[:num_samples], desc=\"Lookahead Accuracy\"):\n        tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n        if tokens.shape[1] < 128:\n            continue\n\n        outputs = model(tokens, use_cache=True, output_attentions=True, return_dict=True)\n        past_kv = outputs.past_key_values\n        seq_len = tokens.shape[1]\n\n        keys, values = get_cache_keys_values(past_kv, 0)\n        positions = torch.arange(seq_len, device=device).unsqueeze(0)\n\n        pred_importance = sip_scorer(keys, values, positions, return_all_lookahead=True)\n\n        actual_attns = []\n        current_kv = past_kv\n        current_pos = seq_len\n\n        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n\n        for step in range(max(lookahead_steps)):\n            position_ids = torch.tensor([[current_pos]], device=device)\n\n            outputs = model(\n                next_token,\n                past_key_values=current_kv,\n                position_ids=position_ids,\n                use_cache=True,\n                output_attentions=True,\n                return_dict=True\n            )\n            current_kv = outputs.past_key_values\n            current_pos += 1\n\n            attn = outputs.attentions[-1]\n\n            if attn.dim() == 4:\n                attn = attn[:, :, -1, :]\n\n            attn = attn[:, :, :seq_len]\n            attn = attn / (attn.sum(dim=-1, keepdim=True) + 1e-8)\n            actual_attns.append(group_heads(attn))\n\n            next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n\n        for step in lookahead_steps:\n            if step <= len(actual_attns) and step <= pred_importance.shape[-1]:\n                pred = pred_importance[:, :, :, step-1].flatten().cpu().numpy()\n                actual = actual_attns[step-1].flatten().cpu().numpy()\n\n                if len(pred) == len(actual) and len(pred) > 1:\n                    pearson_corr, _ = stats.pearsonr(pred, actual)\n                    if not np.isnan(pearson_corr):\n                        results[step]['pearson'].append(pearson_corr)\n\n                    spearman_corr, _ = stats.spearmanr(pred, actual)\n                    if not np.isnan(spearman_corr):\n                        results[step]['spearman'].append(spearman_corr)\n\n                    k = max(1, len(pred) // 4)\n                    pred_topk = set(np.argsort(pred)[-k:])\n                    actual_topk = set(np.argsort(actual)[-k:])\n                    recall = len(pred_topk & actual_topk) / k\n                    results[step]['topk_recall'].append(recall)\n\n    final_results = {}\n    for step in lookahead_steps:\n        if results[step]['pearson']:\n            final_results[step] = {\n                'pearson_mean': np.mean(results[step]['pearson']),\n                'pearson_std': np.std(results[step]['pearson']),\n                'spearman_mean': np.mean(results[step]['spearman']),\n                'spearman_std': np.std(results[step]['spearman']),\n                'topk_recall_mean': np.mean(results[step]['topk_recall']),\n                'topk_recall_std': np.std(results[step]['topk_recall']),\n                'n_samples': len(results[step]['pearson']),\n            }\n\n    return final_results\n\n\n@torch.no_grad()\ndef evaluate_baseline_lookahead(\n    model, tokenizer, texts, \n    lookahead_steps=[1, 4, 8], num_samples=30, device='cuda'\n):\n    num_attention_heads = model.config.num_attention_heads\n    num_kv_heads = getattr(model.config, 'num_key_value_heads', num_attention_heads)\n    head_group_size = num_attention_heads // num_kv_heads\n\n    def group_heads(attn):\n        if head_group_size == 1:\n            return attn\n        if attn.dim() == 3:\n            b, h, s = attn.shape\n            return attn.view(b, num_kv_heads, head_group_size, s).mean(dim=2)\n        elif attn.dim() == 4:\n            b, h, q, k = attn.shape\n            return attn.view(b, num_kv_heads, head_group_size, q, k).mean(dim=2)\n        return attn\n\n    baselines = {\n        'Prefill-Attn': [],\n        'Recency': [],\n        'Random': [],\n    }\n\n    results = {name: {s: [] for s in lookahead_steps} for name in baselines.keys()}\n\n    for text in tqdm(texts[:num_samples], desc=\"Baseline Lookahead\"):\n        tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n        if tokens.shape[1] < 128:\n            continue\n\n        outputs = model(tokens, use_cache=True, output_attentions=True, return_dict=True)\n        past_kv = outputs.past_key_values\n        seq_len = tokens.shape[1]\n\n        prefill_attn = outputs.attentions[-1]\n        if prefill_attn.dim() == 4:\n            prefill_attn = prefill_attn.mean(dim=2)\n        prefill_attn = group_heads(prefill_attn)\n        prefill_pred = prefill_attn.flatten().cpu().numpy()\n\n        recency = torch.arange(seq_len, device=device, dtype=torch.float32) / seq_len\n        recency = recency.unsqueeze(0).unsqueeze(0).expand(1, num_kv_heads, -1)\n        recency_pred = recency.flatten().cpu().numpy()\n\n        random_pred = np.random.rand(num_kv_heads * seq_len)\n\n        actual_attns = []\n        current_kv = past_kv\n        current_pos = seq_len\n        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n\n        for step in range(max(lookahead_steps)):\n            position_ids = torch.tensor([[current_pos]], device=device)\n            outputs = model(next_token, past_key_values=current_kv, position_ids=position_ids,\n                            use_cache=True, output_attentions=True, return_dict=True)\n            current_kv = outputs.past_key_values\n            current_pos += 1\n\n            attn = outputs.attentions[-1]\n            if attn.dim() == 4:\n                attn = attn[:, :, -1, :]\n            attn = attn[:, :, :seq_len]\n            attn = attn / (attn.sum(dim=-1, keepdim=True) + 1e-8)\n            actual_attns.append(group_heads(attn))\n\n            next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n\n        for step in lookahead_steps:\n            if step <= len(actual_attns):\n                actual = actual_attns[step-1].flatten().cpu().numpy()\n\n                for name, pred in [('Prefill-Attn', prefill_pred),\n                                    ('Recency', recency_pred),\n                                    ('Random', random_pred)]:\n                    if len(pred) == len(actual):\n                        corr, _ = stats.spearmanr(pred, actual)\n                        if not np.isnan(corr):\n                            results[name][step].append(corr)\n\n    final = {}\n    for name in baselines.keys():\n        final[name] = {}\n        for step in lookahead_steps:\n            if results[name][step]:\n                final[name][step] = {\n                    'mean': np.mean(results[name][step]),\n                    'std': np.std(results[name][step]),\n                }\n\n    return final\n\n\nprint(\"SIP Lookahead Prediction Accuracy\")\nlookahead_results = evaluate_lookahead_accuracy(\n    model, tokenizer, sip_scorer, test_texts,\n    lookahead_steps=[1, 4, 8], num_samples=50, device=str(device)\n)\n\nprint(f\"\\n{'Step':<8} | {'Pearson':>12} | {'Spearman':>12} | {'Top-25% Recall':>15}\")\nprint(\"-\"*55)\nfor step, metrics in lookahead_results.items():\n    print(f\"{step}-step   | {metrics['pearson_mean']:.3f} +/- {metrics['pearson_std']:.3f} | \"\n        f\"{metrics['spearman_mean']:.3f} +/- {metrics['spearman_std']:.3f} | \"\n        f\"{metrics['topk_recall_mean']:.3f} +/- {metrics['topk_recall_std']:.3f}\")\n\nprint(\"\\nBaseline Comparison (Spearman Correlation)\")\nbaseline_results = evaluate_baseline_lookahead(\n    model, tokenizer, test_texts,\n    lookahead_steps=[1, 4, 8], num_samples=30, device=str(device)\n)\n\nprint(f\"\\n{'Method':<15} | {'1-step':>12} | {'4-step':>12} | {'8-step':>12}\")\nprint(\"-\"*55)\n\nfor name in ['Prefill-Attn', 'Recency', 'Random']:\n    row = f\"{name:<15} |\"\n    for step in [1, 4, 8]:\n        if step in baseline_results.get(name, {}):\n            m = baseline_results[name][step]\n            row += f\" {m['mean']:.3f} +/- {m['std']:.3f} |\"\n        else:\n            row += f\" {'--':>12} |\"\n    print(row)\n\nrow = f\"{'SIP (Ours)':<15} |\"\nfor step in [1, 4, 8]:\n    if step in lookahead_results:\n        m = lookahead_results[step]\n        row += f\" {m['spearman_mean']:.3f} +/- {m['spearman_std']:.3f} |\"\n    else:\n        row += f\" {'--':>12} |\"\nprint(row)\n\nprint(\"\\nAnalysis\")\nfor step in [1, 4, 8]:\n    if step in lookahead_results and 'Prefill-Attn' in baseline_results and step in baseline_results['Prefill-Attn']:\n        sip_corr = lookahead_results[step]['spearman_mean']\n        prefill_corr = baseline_results['Prefill-Attn'][step]['mean']\n        diff = sip_corr - prefill_corr\n        print(f\"  {step}-step: SIP {sip_corr:.3f} vs Prefill-Attn {prefill_corr:.3f} (d={diff:+.3f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 11. Confidence Calibration with Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef evaluate_calibration(\n    model, tokenizer, sip_scorer, texts, \n    num_samples=50, num_bins=10, device='cuda'\n):\n    num_attention_heads = model.config.num_attention_heads\n    num_kv_heads = getattr(model.config, 'num_key_value_heads', num_attention_heads)\n    head_group_size = num_attention_heads // num_kv_heads\n\n    def group_heads(attn):\n        if head_group_size == 1:\n            return attn\n        if attn.dim() == 3:\n            b, h, s = attn.shape\n            return attn.view(b, num_kv_heads, head_group_size, s).mean(dim=2)\n        elif attn.dim() == 4:\n            b, h, q, k = attn.shape\n            return attn.view(b, num_kv_heads, head_group_size, q, k).mean(dim=2)\n        return attn\n\n    all_conf = []\n    all_correct = []\n    all_rank_error = []\n\n    for text in tqdm(texts[:num_samples], desc=\"Calibration\"):\n        tokens = tokenizer(text, return_tensors='pt', max_length=512, truncation=True).input_ids.to(device)\n        seq_len = tokens.shape[1]\n        if seq_len < 128:\n            continue\n\n        outputs = model(tokens, use_cache=True, output_attentions=True, return_dict=True)\n        past_kv = outputs.past_key_values\n\n        keys, values = get_cache_keys_values(past_kv, 0)\n        positions = torch.arange(seq_len, device=device).unsqueeze(0)\n\n        kv_features = sip_scorer.kv_encoder(keys.float(), values.float())\n        pred, conf = sip_scorer.predictor(kv_features, positions, seq_len)\n\n        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n        position_ids = torch.tensor([[seq_len]], device=device)\n\n        outputs = model(\n            next_token,\n            past_key_values=past_kv,\n            position_ids=position_ids,\n            output_attentions=True,\n            return_dict=True\n        )\n\n        actual = outputs.attentions[-1]\n        if actual.dim() == 4:\n            actual = actual[:, :, -1, :]\n        actual = actual[:, :, :seq_len]\n        actual = actual / (actual.sum(dim=-1, keepdim=True) + 1e-8)\n        actual = group_heads(actual)\n\n        pred_step0 = pred[:, :, :, 0]\n        conf_step0 = conf[:, :, :, 0]\n\n        k = max(1, seq_len // 4)\n\n        for h in range(pred_step0.shape[1]):\n            pred_h = pred_step0[0, h, :].cpu().numpy()\n            actual_h = actual[0, h, :].cpu().numpy()\n            conf_h = conf_step0[0, h, :].cpu().numpy()\n\n            pred_topk = set(np.argsort(pred_h)[-k:])\n            actual_topk = set(np.argsort(actual_h)[-k:])\n\n            for i in range(seq_len):\n                pred_is_topk = i in pred_topk\n                actual_is_topk = i in actual_topk\n                is_correct = (pred_is_topk == actual_is_topk)\n\n                all_conf.append(conf_h[i])\n                all_correct.append(float(is_correct))\n\n                pred_rank = np.argsort(np.argsort(pred_h))[i] / seq_len\n                actual_rank = np.argsort(np.argsort(actual_h))[i] / seq_len\n                all_rank_error.append(abs(pred_rank - actual_rank))\n\n    return {\n        'confidence': np.array(all_conf),\n        'correct': np.array(all_correct),\n        'rank_error': np.array(all_rank_error),\n    }\n\n\ndef compute_ece(confidence, accuracy, num_bins=10):\n    bins = np.linspace(0, 1, num_bins + 1)\n    ece = 0.0\n    bin_stats = []\n\n    for i in range(num_bins):\n        in_bin = (confidence > bins[i]) & (confidence <= bins[i+1])\n        bin_size = in_bin.sum()\n\n        if bin_size > 0:\n            bin_acc = accuracy[in_bin].mean()\n            bin_conf = confidence[in_bin].mean()\n            bin_error = abs(bin_acc - bin_conf)\n            ece += (bin_size / len(confidence)) * bin_error\n            bin_stats.append({\n                'bin': f\"({bins[i]:.1f}, {bins[i+1]:.1f}]\",\n                'count': bin_size,\n                'avg_conf': bin_conf,\n                'avg_acc': bin_acc,\n                'error': bin_error,\n            })\n        else:\n            bin_stats.append({\n                'bin': f\"({bins[i]:.1f}, {bins[i+1]:.1f}]\",\n                'count': 0,\n                'avg_conf': 0,\n                'avg_acc': 0,\n                'error': 0,\n            })\n\n    return ece, bin_stats\n\n\ndef apply_temperature_scaling(confidence, correct, num_iters=50):\n    best_ece = float('inf')\n    best_temp = 1.0\n\n    for temp in np.logspace(-1, 1, 50):\n        scaled_conf = 1 / (1 + np.exp(-np.log(confidence / (1 - confidence + 1e-8)) / temp))\n        scaled_conf = np.clip(scaled_conf, 0.01, 0.99)\n        ece, _ = compute_ece(scaled_conf, correct)\n        if ece < best_ece:\n            best_ece = ece\n            best_temp = temp\n\n    return best_temp, best_ece\n\n\nprint(\"Collecting Calibration Data\")\ncalib_data = evaluate_calibration(\n    model, tokenizer, sip_scorer, test_texts,\n    num_samples=50, device=str(device)\n)\n\nconfidence = calib_data['confidence']\ncorrect = calib_data['correct']\nrank_error = calib_data['rank_error']\n\nprint(f\"Collected {len(confidence)} confidence-accuracy pairs\")\nprint(f\"Mean confidence: {confidence.mean():.3f}\")\nprint(f\"Mean accuracy: {correct.mean():.3f}\")\nprint(f\"Mean rank error: {rank_error.mean():.3f}\")\n\nprint(\"\\nECE Before Temperature Scaling\")\nece_before, bins_before = compute_ece(confidence, correct)\nprint(f\"ECE: {ece_before:.4f}\")\n\nprint(f\"\\n{'Bin':<15} | {'Count':>8} | {'Avg Conf':>10} | {'Avg Acc':>10} | {'Error':>8}\")\nprint(\"-\"*60)\nfor b in bins_before:\n    if b['count'] > 0:\n        print(f\"{b['bin']:<15} | {b['count']:>8} | {b['avg_conf']:>10.3f} | {b['avg_acc']:>10.3f} | {b['error']:>8.3f}\")\n\nprint(\"\\nTemperature Scaling\")\noptimal_temp, ece_after = apply_temperature_scaling(confidence, correct)\nprint(f\"Optimal temperature: {optimal_temp:.4f}\")\nprint(f\"ECE after scaling: {ece_after:.4f}\")\nprint(f\"ECE improvement: {((ece_before - ece_after) / ece_before) * 100:.1f}%\")\n\nscaled_conf = 1 / (1 + np.exp(-np.log(confidence / (1 - confidence + 1e-8)) / optimal_temp))\nscaled_conf = np.clip(scaled_conf, 0.01, 0.99)\n_, bins_after = compute_ece(scaled_conf, correct)\n\nprint(f\"\\n{'Bin':<15} | {'Count':>8} | {'Avg Conf':>10} | {'Avg Acc':>10} | {'Error':>8}\")\nprint(\"-\"*60)\nfor b in bins_after:\n    if b['count'] > 0:\n        print(f\"{b['bin']:<15} | {b['count']:>8} | {b['avg_conf']:>10.3f} | {b['avg_acc']:>10.3f} | {b['error']:>8.3f}\")\n\nprint(\"\\nCalibration Summary\")\nprint(f\"ECE (before): {ece_before:.4f}\")\nprint(f\"ECE (after):  {ece_after:.4f}\")\nprint(f\"Optimal T:    {optimal_temp:.4f}\")\n\ncalib_results = {\n    'ece_before': ece_before,\n    'ece_after': ece_after,\n    'optimal_temperature': optimal_temp,\n    'mean_confidence': float(confidence.mean()),\n    'mean_accuracy': float(correct.mean()),\n}"
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 12. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Final Results Summary\")\nprint(f\"Model: TinyLlama-1.1B\")\nprint(f\"SIP Parameters: {sum(p.numel() for p in sip_scorer.parameters()):,}\")\n\nif 'pregeneration_results' in dir():\n    print(f\"\\nBaseline (Full Cache): {baseline_ppl:.2f}\")\n    print(f\"\\n{'Method':<20} |\", end=\"\")\n    for ratio in [0.10, 0.25, 0.50, 0.75]:\n        print(f\" {int(ratio*100):>5}% |\", end=\"\")\n    print(\"\")\n    print(\"-\"*55)\n\n    method_order = ['SIP (Ours)', 'Prefill-Attn', 'Position-Heuristic',\n                    'Expected-Attn', 'TRIM-KV', 'Random']\n    for method in method_order:\n        if method in pregeneration_results:\n            row = f\"{method:<20} |\"\n            for ratio in [0.10, 0.25, 0.50, 0.75]:\n                if ratio in pregeneration_results[method]:\n                    ppl = pregeneration_results[method][ratio]\n                    row += f\" {ppl:>5.2f} |\"\n                else:\n                    row += f\" {'--':>5} |\"\n            print(row)\n\nif 'multiseed_results' in dir():\n    print(f\"\\nMulti-Seed Results (5 seeds, 95% CI)\")\n    print(f\"\\n{'Method':<20} |\", end=\"\")\n    for ratio in [0.10, 0.25, 0.50, 0.75]:\n        print(f\" {int(ratio*100):>12}% |\", end=\"\")\n    print(\"\")\n    print(\"-\"*70)\n\n    for method in ['SIP (Ours)', 'Prefill-Attn', 'Position-Heuristic', 'Random']:\n        if method in multiseed_results:\n            row = f\"{method:<20} |\"\n            for ratio in [0.10, 0.25, 0.50, 0.75]:\n                if ratio in multiseed_results[method]:\n                    m = multiseed_results[method][ratio]\n                    row += f\" {m['mean']:.2f}+/-{m['ci_95']:.2f} |\"\n                else:\n                    row += f\" {'--':>12} |\"\n            print(row)\n\nif 'lookahead_results' in dir():\n    print(f\"\\nLookahead Prediction Accuracy\")\n    print(f\"\\n{'Step':<8} | {'Spearman':>12} | {'Top-25% Recall':>15}\")\n    print(\"-\"*45)\n    for step in [1, 4, 8]:\n        if step in lookahead_results:\n            m = lookahead_results[step]\n            spearman = m.get('spearman_mean', m.get('mean', 0))\n            spearman_std = m.get('spearman_std', m.get('std', 0))\n            recall = m.get('topk_recall_mean', 0)\n            recall_std = m.get('topk_recall_std', 0)\n            print(f\"{step}-step   | {spearman:.3f} +/- {spearman_std:.3f} | {recall:.3f} +/- {recall_std:.3f}\")\n\nif 'calib_results' in dir():\n    print(f\"\\nConfidence Calibration\")\n    print(f\"  ECE (before): {calib_results['ece_before']:.4f}\")\n    print(f\"  ECE (after):  {calib_results['ece_after']:.4f}\")\n    print(f\"  Optimal T:    {calib_results['optimal_temperature']:.4f}\")\nelif 'ece_before' in dir():\n    print(f\"\\nConfidence Calibration\")\n    print(f\"  ECE (before): {ece_before:.4f}\")\n    print(f\"  ECE (after):  {ece_after:.4f}\")\n    print(f\"  Optimal T:    {optimal_temp:.4f}\")\n\ndef to_serializable(obj):\n    if isinstance(obj, dict):\n        return {str(k): to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [to_serializable(v) for v in obj]\n    elif isinstance(obj, (np.floating, np.integer)):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif hasattr(obj, 'item'):\n        return obj.item()\n    else:\n        return obj\n\nfinal_results = {\n    'model': 'TinyLlama-1.1B',\n    'sip_parameters': sum(p.numel() for p in sip_scorer.parameters()),\n    'training_config': TRAINING_CONFIG,\n}\n\nif 'baseline_ppl' in dir():\n    final_results['baseline_perplexity'] = baseline_ppl\n\nif 'pregeneration_results' in dir():\n    final_results['perplexity_results'] = to_serializable(pregeneration_results)\n\nif 'multiseed_results' in dir():\n    final_results['multiseed_results'] = to_serializable(multiseed_results)\n\nif 'qa_results' in dir():\n    final_results['qa_accuracy'] = to_serializable(qa_results)\n\nif 'needle_results' in dir():\n    final_results['needle_in_haystack'] = to_serializable(needle_results)\n\nif 'ablation_results' in dir():\n    final_results['ablation_study'] = to_serializable(ablation_results)\n\nif 'lookahead_results' in dir():\n    final_results['lookahead_accuracy'] = to_serializable(lookahead_results)\n\nif 'calib_results' in dir():\n    final_results['calibration'] = to_serializable(calib_results)\nelif 'ece_before' in dir():\n    final_results['calibration'] = {\n        'ece_before': float(ece_before),\n        'ece_after': float(ece_after),\n        'optimal_temperature': float(optimal_temp),\n    }\n\nif 'history' in dir():\n    final_results['training_history'] = to_serializable(history)\n\nwith open('sip_comprehensive_results.json', 'w') as f:\n    json.dump(final_results, f, indent=2)\nprint(\"\\nResults saved to sip_comprehensive_results.json\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook provides comprehensive experiments for our negative results paper on learned KV cache compression.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **SIP does not outperform simple heuristics** across any retention level or task\n",
    "2. **Position-based methods** (sinks + recent) win at aggressive compression (10-25%)\n",
    "3. **Prefill attention** wins at moderate compression (50-75%)\n",
    "4. **SIP ≈ Random** — no statistically significant difference in multi-seed evaluation\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "For practitioners deploying KV cache compression:\n",
    "- **Aggressive compression (10-25%):** Use Position-Heuristic (keep sinks + recent tokens)\n",
    "- **Moderate compression (50-75%):** Use Prefill-Attn (attention from prompt processing)\n",
    "- **Learned methods provide no benefit** in our evaluation setting\n",
    "\n",
    "### Implications for Research\n",
    "\n",
    "Our results suggest that for **non-query-aware** importance scoring under fixed-budget compression:\n",
    "- Simple heuristics capture most predictable signal\n",
    "- Marginal information in KV representations beyond position appears limited\n",
    "- The circular dependence between future queries and generation may be a fundamental barrier\n",
    "\n",
    "See the paper for full analysis and discussion of the query-aware vs non-query-aware distinction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}